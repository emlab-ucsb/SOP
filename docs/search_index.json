[["index.html", "emLab Standard Operating Procedures Overview", " emLab Standard Operating Procedures Environmental Markets Lab (emLab) 2025-12-01 Overview This reference guide describes standard operating procedures (SOP) for emLab projects. This SOP is organized into 5 different sections: (1) File Structure, (2) Data, (3) Code, (4) High Performance Computing, and (5) Reports and Publications. Section 1 covers file structure and storage; Section 2 describes file naming and metadata standards, as well as what the emLab data directory is composed of; Section 3 highlights best practices for coding; Section 4 covers high performance computing; and Section 5 outlines aspects of the publication process from emLab affiliations to preparing a public GitHub repository. "],["1-file-structure.html", "1 File Structure", " 1 File Structure This section details emLab’s organizational stucture for Google Docs/Sheets/Slides stored on Google Shared Drive, data and other file types stored emLab’s GRIT data storage space (which we will simply refer to as “GRIT”), and code stored on GitHub. An important note: while the file structures of our Google Shared Drive and GRIT storage spaces are similar, they contain fundamentally different types of files. For the Google Shared Drive, we should store only collaborate Google Docs/Sheets/Slides. For the GRIT data storage space, we should store everything else (e.g., data, media, Micorosft Office files, PDFs, etc) "],["1.1-folder-naming.html", "1.1 Folder Naming", " 1.1 Folder Naming There are some general style conventions that should be used when naming folders. For any new folder, be descriptive but concise, avoid spaces, avoid uppercase (or camel-case), and avoid special characters other than -, such as = \\ / : * ? \" ' &lt; &gt; |. Words should be separated with -, so an example folder name could be blue-paradox-paper. When possible, avoid unnecessary deep file nesting (i.e., a folder within a folder within a folder within a folder…etc) to keep file paths relatively short and easy to navigate. This folder naming convention should be used for any folder added to the Google Shared Drive, GRIT, or GitHub. "],["1.2-google-shared-drive.html", "1.2 Google Shared Drive", " 1.2 Google Shared Drive The Google Shared Drive and GRIT storage space share a very similar folder structure. The main difference is that the Google Shared Drive does not have any data folders, since all data is stored at GRIT. And of course, all files on Google Shared Drive are only collaborative Google Docs/Sheets/Slides. Everything else is stored at GRIT. 1.2.1 General Structure Google Drive |__ My Drive | |__ ... whatever files you have on your personal Google Drive ... |__ Shared drives |__ emLab |__ central-emlab-resources |__ communications |__ projects The emLab Google Shared Drive is organized into three main folders: central-emlab-resources: includes meeting and event information, project management guidelines, onboarding materials, information about travel reimbursements, strategy, computing, and the team roster communications: includes the blog schedule, publication and media tracking, and any Google Docs/Sheets/Slides relating to communications projects: includes information on past (archive) and current projects 1.2.2 Project Folder Structure Google Drive |__ Shared drives |__ emLab |__ projects |__ archived-projects |__ current-project | |__ example-project | | |__ deliverables | | |__ grant-eporting | | |__ meetings-and-events | | |__ presentations | | |__ project-materials Each project folder must contain at least the following 5 folders: deliverables: final reports, paper manuscripts, other final deliverables not related to data outputs grant-reporting: grant reports for funders meetings-and-events: meeting notes, agendas, documentation for workshop/event planning presentations: any presentations created for the project project-materials: everything else that does not fit into one of these folders (i.e. drafts of methods, literature review, etc.) From here, each project can add additional folders or sub-folders as needed. Just ensure that only collaborative Google Docs/Sheets/Slides are stored on Google Shared Drive. All other files should be stored on GRIT. "],["1.3-grit-data-storage-space.html", "1.3 GRIT data storage space", " 1.3 GRIT data storage space Generally, the GRIT data storage space has a very similar structure to the Google Shared Drive, except it also has folders for data. 1.3.1 General Structure |__ emLab |__ central-emlab-resources |__ communications |__ data |__ projects The emLab GRIT data directory is organized into five main folders: central-emlab-resources: includes meeting and event information, project management guidelines, onboarding materials, information about travel reimbursements, strategy, computing, and the team roster communications: includes the blog schedule, Adobe design projects, PowerPoint templates, photo repository, emLab logos, and publication and media tracking data: includes the centralized emLab data directory of the commonly used datasets we work with across projects (see Sections 1.3.3.2 and 2.2.2 for more on this) projects: includes data and files on past (archive) and current projects 1.3.2 Project Folder Structure |__ emLab |__ projects |__ archived-projects |__ current-project | |__ example-project | | |__ data | | |__ deliverables | | |__ grant-eporting | | |__ meetings-and-events | | |__ presentations | | |__ project-materials Each project folder on GRIT must contain the following 6 folders: data: This data folder will contain a data_overview spreadsheet and all of the intermediate datasets as well as output datasets associated with the project (see Section 2.2.3 for more on this). Be sure to also add a copy of your final datasets to the emlab/datadata directory. deliverables: final reports, paper manuscripts, other final deliverables not related to data outputs grant-reporting: grant reports for funders meetings-and-events: meeting notes, agendas, documentation for workshop/event planning presentations: any presentations created for the project project-materials: everything else that does not fit into one of these folders (i.e. drafts of methods, literature review, etc.) From here, each project can add additional folders or sub-folders as needed. 1.3.3 Data Directories As stated above, there are two locations in which data can be stored. The two locations are both on GRIT, and are either: 1) project-specific data (e.g.,emLab/current-projects/example-project/data); or 2) the commonly used data shared across emLab projects (emLab/data) . This may seem confusing and redundant, but this section explains the differences between these two locations. As a short summary: example-project/data will contain all raw, cleaned, intermediate, and output data files for a given project, and will be used as the “workspace” while the project develops. On the other hand, emlab/data contains only (raw) input and output data from a finalized project. It therefore contains datasets that researchers envision may eventually be helpful to other researchers and that may be commonly used across multiple projects. More detail is provided in the subsequent sections. 1.3.3.1 example-project/data All data used in a project should live in this project-specific repository. To help keep track of project data, we highly recommend creating and maintaining a data_overview Google sheet on the Google Shared Drive (see Section 2.2.3 for more information). This overview spreadsheet will provide a centralized summary of the data inputs and outputs for a project, and also allow teams to keep track of the status of adding the data to the emlab/data folder and creating the necessary metadata documentation. This repository will typically contain subfolders for raw, processed (or clean), and output, although each team might make slight modifications to this structure to suit their needs. To illustrate how each of these subfolders might be used, consider the following. A team may receive data from partners, extract data from external sources, compile survey responses, create a new dataset from a literature review, or use results from previous projects as input. These data are termed “raw data” and should never be directly modified - all of the errors, mistakes, and gremlins should be kept in the original versions. Instead, they should be processed / cleaned, and then exported as “clean data” that is actually used in analyses. The script used to do the processing / cleaning then acts as a reproducable record of everything that was done to the raw data. Suppose that a team working in Montserrat is tasked to perform a stock assessment on lobster populations and receives a database of lobster landings from the government. These data are stored as an excel spreadsheet, and will surely contain many mistakes that need to be fixed prior to running anly analyses. The team will clean the data (preferabily, using a reproducible script), and then export a new version of the data in which the mistakes have been fixed. The team will then perform the stock assessment, and produce results before reporting back. Therefore, the project-level data folder could be subdivided into raw, clean, and output folders. The first one will contain the excel file recieved from the government. The second folder will contain the cleaned data (perhaps exported as a csv), which can then be used as input for analyses within this project. The output folder will then contain the stock assessment results that might be relevant to other projects. |__ emLab |__ projects |__ current-rojects |__ montserrat-project | |__ Data | |__ raw | | |__ lobster_landings_nov_2012.xslx | |__ clean | | |__ lobster_landings_nov_2012.csv | |__ output | |__ lobster_stock_assessment.csv As stated above, since the output folder could contain information relevant to other projects, this data should be made available to other emLab projects once the project is complete. To do this, any output data (and raw data if it is not already there) should be moved to the emlab/data folder, as described below. 1.3.3.2 emlab/data As a general rule, this folder contains all data used and produced by emLab projects. The idea is to make it easier for people to find data that has been used in previous projects, as well as to use previous results as inputs for new projects. In other words, it is the place to store data that could be used commonly across multiple projects. Please see Section @ref(#emlab-data-directory) for an overview of metadata that should be included with each dataset. To illustrate types of data that should be in the emlab/data folder, consider the following. The RAM Legacy stock assessment database is key to many projects, and was used as input in the Costello et al. 2016 “upsides” paper. The “upsides database” is an output from the Costello paper, which has then been used as input for other projects. Therefore, the emlab/data folder contains separate folders for both the RAM and upsides datasets. This large central data repository has the potential to become messy. Therefore, it is important to follow some key guidelines to store the data. All datasets in this folder should be contained within their own folders that include at minimum the data and metadata files. For example, a file structure for the two datasets mentioned above might be: |__ emLab |__ data |__ upsides | |__ _readme_upsides.txt [the metadata] | |__ upsides.csv [the data] |__ ram | |__ _readme_RAM.txt [the metadata] | |__ RAM v4.10 [the data] | |__ RAM v4.15 [the data] | |__ RAM v4.25 [the data] | |__ RAM v4.40 [the data] |__ ... other data sets ... In the above example, the folder containing the upsides database is relatively straightforward with the metadata file and a single csv file. However, the folder containing the RAM database is more complicated as this is a dataset that is re-released every so often as a new version. Specific guidelines for organizing different types of data within the emlab/data folder are discussed in detail in Section 2. "],["1.4-github-structure.html", "1.4 GitHub Structure", " 1.4 GitHub Structure 1.4.1 Project Repository Structure The structure of each emlab repository on GitHub will likely vary depending on the needs of the project, but the following structure is suggested as a starting point. A documents (or docs) folder may be useful for storing code files that are used to generate text-based documents or presentations. Types of files that might live here include things like markdown files. A results folder may be useful for storing plots or other types of results generated by the project. Some discretion needs to be used here, as some results may actually be considered to be “processed” or “output” data. However, results in the form of figures or workspace image files might live here. A scripts folder may be useful for storing the code files that do everything from processing the raw data to running the analysis and generating outputs. A functions folder may be useful for storing the code files in which functions that are used by many scripts many be stored. Different types of projects may require more or fewer folders and these are only meant to act as suggestions. Regardless, the structure of the repository should be sufficiently organized such that it can be easily navigated and understood by others by the time the project is completed. 1.4.2 A repo inside a repo Sometimes, a project may have more than one paper or analysis sections. On some corner scenarios, we might want to have multiple “paper folders” within a “project folder”. This would imply that we will have a repo inside a repo. If that is something that makes sense for you, your project, and your team, then git submodules are your solution. If you want to read more on when / how to use submodules, visit the documentation page here. Including submodules in your workflow is simple. Here’s an example. you are working on a big project called “Blue Future”. The project has six PIs, 13 Research Specialists, two PostDocs, and three PhD Students. After a long kick-off meeting, the team realizes that the project will produce two papers and a ShinyApp. You are all determined to keep everything on the same folder, but correctly categorized and organized. As such, you go to GitHub and create the following four repositories: Blue Future Paper 1 Paper 2 ShinyApp You’ll clone the Blue Future repo into your comuter, using the usual: git clone https://github.com/emlab-ucsb/BlueFuture.git Now, instead of cloning the repos for each paper and the app into their own folder, you’ll navigate into your local BlueFuture folder. Then, instead of cloning them there, you can just do: git submodule add https://github.com/emlab-ucsb/Paper1.git This will clone the Paper 1 repo, but not without first telling the BlueFuture repo about it (just so that you don’t end up tracking things twice). You can repeat the operation for Paper2 and ShinyApp. That’s it! "],["2-data.html", "2 Data", " 2 Data Data should be managed and shared properly to make it useful in research, to promote transparency and facilitate reproducibility, and to ensure the credibility of research. This includes such practices as transforming data into a tidy format, storing data in open file formats, and providing data documentation. All emLab data, unless restricted, should be stored in the emLab Shared Drive (location dependent on type of data, see Section 2.2.3). Recommended readings: Borer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Ellis, Shannon E., and Jeffrey T. Leek. 2018. “How to Share Data for Collaboration.” The American Statistician 72 (1): 53–57. https://doi.org/10.1080/00031305.2017.1375987. Goodman, Alyssa, Alberto Pepe, Alexander W. Blocker, Christine L. Borgman, Kyle Cranmer, Merce Crosas, Rosanne Di Stefano, et al. 2014. “Ten Simple Rules for the Care and Feeding of Scientific Data.” PLOS Computational Biology 10 (4): e1003542. https://doi.org/10.1371/journal.pcbi.1003542. Hart, Edmund M., Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H. Woo, Naupaka B. Zimmerman, and Jeffrey W. Hollister. 2016. “Ten Simple Rules for Digital Data Storage.” PLOS Computational Biology 12 (10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097. Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (March): 160018. https://doi.org/10.1038/sdata.2016.18. Also see the resources available at DataONE. "],["2.1-data-file-naming.html", "2.1 Data File Naming", " 2.1 Data File Naming “Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread” - Hadley Wickham Coding styles guides provide teams with a shared language that promotes consistency, improves collaboration, and makes code easier to write as it simplifies the number of decisions we need to make on a daily basis (e.g., do I use - or _ to name this file? ). All style guides are opinionated and while they all intend to make code easier to read and write, some decisions are arbitrary. Thus, instead of us at emLab creating our own, we want to adopt the “proven and tested” Tidyverse style guide. This styles guide has good advice not only for naming files but also for naming functions, objects, and general best coding practices. We encourage all emLab members to read the guide and work towards adopting it. When it comes to file naming, best practices include: Names should be descriptive and meaningful. Many coding interfaces now have autocompletion tools, so the length of the filename is less of a concern. Avoid spaces and upper case letters. Some operating systems are case sensitive so in the interest of collaboration, let’s use only only lower case. Use _ to separate words in filenames. Do not use -, ., or any other special characters such as = \\ / : * ? \" ' &lt; &gt; |. Strive to use verbs for function names. If files should be run in a particular order, prefix them with numbers. If you’ll have more than 10 files left pad with zero (e.g., 01_get_data.R) "],["2.2-metadata.html", "2.2 Metadata", " 2.2 Metadata Metadata is data about your data. It includes information about your data’s content, structure, authors, and permissions to make your data interpretable and usable by your future self and others. EVERY data file should be accompanied by a metadata file. This includes files that people tend to overlook or think are not useful for the broader team. For example, if you’re using Google Sheets to keep track of literature or data reviews for a specific project, these documents should also have some form of accompanying metadata. 2.2.1 Metadata Standards We use “readme” style metadata, named _readme_datafilename, and stored in the same folder as the data file. Create one readme file for each data file. Download and use this template to create your readme file (when one is not already available). Name the readme _readme_datafilename and save as a text file. Format the readme so it is easy to understand (use bullets, break up information, etc.) Use a standardized date format (YYYY-MM-DD) We acknowledge that it may not always be feasible to draft robust metadata immediately when a dataset is first uploaded to the emlab/data folder. In this case, a minimal readme file can be created as a temporary placeholder that contains the following information: your name, contact info, a very brief (1-2 lines) description of the data, and a note on how you obtained them. Please refer to the Data Directory subheading for further instruction on how to incorporate this form of temporary documentation into the emLab Data Directory. For files that may be considered more “internal notes” than datasets (Google Sheets example mentioned above), please ensure that some sort of metadata is present. One alternative to a “readme” file is to create an extra tab on the Google Sheet labeled “metadata”. Here, you can include information on the column names (column 1) and their definitions (column 2). This allows collaborators and team members to easily interpret the columns and use the dataset appropriately. 2.2.2 emLab common data directory All commonly used emLab data is stored in subfolders of the Data folder on the emLab GRIT storage space (emlab/data). To document these data, we use the emLab Data Directory that includes key, standardized information from each readme metadata file. Every data file in the emlab/data folder has a record (row) in the emLab Data Directory. The emLab Data Directory file contains two sheets: (1) Data directory (the record and standardized documentation for each data file); (2) Metadata (information needed to populate the Data Directory, i.e. the meta-metadata) In the case of placeholder metadata (as described in the Metadata section), only the following columns should be filled out: folder, filename, contact, and summary. This (mostly blank) row serves two purposes: 1) it retains some of the searchability function for that dataset and 2) it serves as a visual reminder that those datasets are in need of more robust metadata development. Column Description Domain Climate/Energy; Land; Ocean; General; Other [drop down menu] Description A few word description (e.g. SST US 2017); max 5 words Folder Name of folder containing data Filename Name of data Year Year of publication Version Sub category of year; NA if not applicable Project Project name that used these data (can have multiple listings) or ‘General’ if widely used (e.g. FAO data), hyperlinked to Google Drive/Box folder Code Link to Github repo or wherever code is stored Data Stage raw’ if raw data; ‘final input’ for the input data used for the analysis; ‘output’ for what was used for the project and/or published [drop down menu] Filetype File extension (e.g. csv; tif; rds); note: do note include ‘.’ Citation Hyperlinked reference to publication or online resource or contact for individual/group data author URL Link to original data source Extent global; regional; national; local [drop down menu] Resolution Resolution of spatial data (in degrees) Permissions open = open source/open access; restricted = need author permission; secure = confidential data and likely involves a DUA or NDA [drop down menu] Start year Data set start year; numeric End year Data set end year; numeric Source e.g. emLab; FAO; Rare Contact Name and email of contact person in emLab who used/stored data emLab reference Hyperlinked reference to emLab publication using data (can be NA) Keywords e.g. fisheries; fire; utilities; property value; VDS; MPA; oceanography; temperature; habitat; biodiversity (up to 5 per entry, separated by semi-colons) Summary Brief description of the data (1-2 sentences). Include years for timeseries; location/spatial extent for spatial data; key variables; resolution; sampling frequency; species; etc. Notes Other relevant information about data. Initial your entry (e.g. if it was processed (e.g. subset from a larger dataset); what specifically was done; are there suspicious data points?; note if there are issues; etc.) Any time you add a new dataset to the shared emLab data folder and directory, please message the #data-streamlining Slack channel so that others on the team know about the new dataset. 2.2.3 Project-specific data directories We highly recommend that research teams create a data_overview spreadsheet for keeping track of project-related data (i.e. a separate Google Sheet stored in the project’s Google Shared Drive folder). This centralized document can be used to document project-relevant information and communicate to team members datasets that have already been saved. This document can then be used to guide and simplify data migration to the emLab Data Directory once the project is complete. Suggested attributes include: File name Folder name Source of data Link where data was downloaded Description of data Name of the researcher who downloaded the data Data directory entry (complete, in progress, not started, etc.) Metadata sheet (complete, in progress, not started, etc.) "],["2.3-accessing-emlab-data-in-r.html", "2.3 Accessing emLab data in R", " 2.3 Accessing emLab data in R This section will go over how to access data stored in the emLab data directories (either the common emLab data directory, or project-specific data directories) using R. We provide instructions for accessing data using R on your personal local machine, or on one of emLab’s GRIT servers. Accessing GRIT data on your local machine requires you to use the Nextcloud desktop app and set it up as described in the emLab manual. If you haven’t done so already, please complete that section. The GRIT data storage space has already been linked to our emLab quebracho server (and will eventually be to our forthcoming sequoia server). Set the base data directory at the top of your code to the location directory lives. You should be able to use the following code snippet to do this, which should work on either your local machines, or on a GRIT server such as quebracho or sequoia. Note that this code will automatically generate the correct directory path if using a GRIT server or a local Mac or Windows machine. If using a local Linux machine, you will need to manually modify this to incorporate your unique user name so that it matches the directory entered in Step 9 above. # First determine if system is quebracho or sequoia, our GRIT servers. If so, set directory appropriately data_directory_base &lt;- ifelse(Sys.info()[&quot;nodename&quot;] == &quot;quebracho&quot; | Sys.info()[&quot;nodename&quot;] == &quot;sequoia&quot;, &quot;/home/emlab&quot;, # Otherwise, set the directory for local machines based on the OS # If using Mac OS, the directory will be automatically set as follows ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Darwin&quot;, &quot;/Users/Shared/nextcloud/emLab&quot;, # If using Windows, the directory will be automatically set as follows ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Windows&quot;, &quot;G:/Shared\\ drives/nextcloud/emLab&quot;, # If using Linux, will need to manually modify the following directory path based on their user name # Replace your_username with your local machine user name &quot;/home/your_username/Nextcloud&quot;))) Now set the project-specific directory. For example: data_directory_project &lt;- file.path(data_directory_base, &quot;projects/current-projects/test-project-nextcloud/data&quot;) You should now be able to read and write data into the project data directory. For example: penguins &lt;- read.csv(file.path(data_directory_project, &quot;raw/penguins.csv&quot;)) Note that you can also always navigate to the data directory in the RStudio viewer pane. To do so, simply click the 3 dots in the viewer pane, and type in the appropriate path. "],["2.4-tidy-data.html", "2.4 Tidy Data", " 2.4 Tidy Data As researchers, we work with data from many different sources. Often these data are messy. One of the first steps of any analysis is to clean any available raw data so that you can make simple visualizations of the data, calculate simple summary statistics, look for missing or incorrect data, and eventually proceed with more involved analyses or modeling. As part of the data cleaning process, we recommend getting all data into a “tidy” format (also known as “long” format, as opposed to “wide” format). According to the Tidy Data Guide by Garrett Grolemund and Hadley Wickham, tidy data is defined as: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. These three features of tidy data can be seen in the following figure, also from the Tidy Data Guide: Once data are in this format, it makes subsequent visualization and analysis much easier. If you’ve ever worked with a file that has a separate column for each year (an example of “wide” format data), you know how hard that type of data format is to work with! As always, we recommend keeping a backup copy of the raw data you obtained from the original source, and using a reproducible script for transforming these data into a tidy data format. 2.4.1 Recommended Resources We highly recommend the chapter on Tidy Data from the book R for Data Science by Garrett Grolemund and Hadley Wickham. This guide is geared towards R users and provides helpful tips for transforming and working with data in R, but the concepts should be broadly applicable to other languages as well. For tips specific to Python, we recommend a blogpost by Jean-Nicholas Hould titled Tidy Data in Python. "],["2.5-data-formats.html", "2.5 Data Formats", " 2.5 Data Formats Data should preferably be stored and/or archived in open formats. Open formats maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them. The exact format will vary depending on the file type, but some common examples are comma separated value (.csv) as opposed to closed format Excel spreadsheets (.xls) for tabular data, and .pdf or .odt (Open document format for office applications) as opposed to Word documents for final versions of reports. When saving final geospatial products, we recommend geopackage (.gpkg) which has several advantages over the commonly used shapefile format (more here): Single file Open format (shapefile is ESRI-dependent) Fewer limitations: e.g. no limits on attribute file names (shapefile has max. 254 characters), max file size 140 TB! (shapefile max. 4GB). Geopackage files can be written and read in R, QGIS, and ArcGIS. "],["2.6-data-use-agreements-and-confidential-data.html", "2.6 Data Use Agreements and Confidential Data", " 2.6 Data Use Agreements and Confidential Data 2.6.1 The process for establishing a Data Use Agreement or Non-Disclosure Agreement At project launch, the project manager and the rest of the project team should determine if a Data Use Agreement (DUA; common) or Non-Disclosure Agreement (NDA; not common) is necessary. Any project that will involve the use and sharing of data that is not publicly available should establish a DUA or NDA. Start this process right away. DUA is preferred if possible; only do NDA if necessary or requested by partner. These agreements should go through UCSB’s Office of Technology &amp; Industry Alliances (TIA). This page provides guidelines for establishing a DUA. The first step in establishing a DUA is to fill out a DUA Request Form and send it to Jenna Nakano (nakano@tia.ucsb.edu). This page provides guidelines for establishing an NDA. The first step in establishing an NDA is to fill out a NDA Request Form and send it to Jenna Nakano (nakano@tia.ucsb.edu). CC emLab’s Amanda Kelley (aekelley@ucsb.edu) on all emails relating to the DUA or NDA. Once a request form has been sent, TIA will help produce standardized agreements that are based on answers to these forms. Alternatively, TIA can also review DUAs or NDAs that partners share. 2.6.2 Data storage options emLab is generally happy to work with partners to determine the most appropriate method for data storage. Some partners may have specific data storage requirements that will be laid out in the DUA or NDA. If the DUA or NDA does not have specific data storage requirements, we recommend one of the following three approaches depending on how sensitive the data are: For data that are not confidential or sensitive, data should be stored in a project-specific directory on the emLab Team Drive. Only emLab PIs and full-time emLab staff will have default access to this data directory. Any additional access for postdocs, students, or other external collaborators will only be granted on an as-needed basis and only after the collaborator has read the DUA or NDA. This option is used for the vast majority of emLab projects. For confidential or sensitive data, the primary recommended approach is the UCSB Knot Cluster through the Center for Scientific Computing. We recommend using this approach if your DUA or NDA allows for it. To set this up, use the request form here. Nathan (Fuzzy) Rogers (Research Computing Administrator, fuz@mrl.ucsb.edu) and Paul Weakliem (CNSI Research Computing Support, weakliem@cnsi.ucsb.edu) are good resources for questions. Anyone storing sensitive data with the knot cluster should ensure that UCSB locks the data so that they remain private. For confidential or sensitive data, the second option is storing data on the Secure Compute Research Environment (SCRE) at UCSB’s North Hall Data Center. The SCRE “is a private, secure, virtual environment for researchers to remotely analyze sensitive data, create research results, and output results and analyses.” We only recommend this approach if your DUA or NDA requires data be stored in a secure facility like the North Hall Data Center Setting up an SCRE gives you access to a secure virtual desktop that comes pre-loaded with applications such as R and R Studio. You can make a request for an SCRE using the request form here. UCSB IT will help set this up. You can follow up with questions at scre-support@ets.ucsb.edu Requests are usually fulfilled within one week. Further information can be found in the SCRE user guide. Jennifer Mehl (Information Security Analyst, jennifer.mehl@ucsb.edu) is another good resource for questions. The SCRE has a number of important limitations: it is relatively slow; it is very difficult to access for non-UCSB collaborators; it can only be set up after the NDA/DUA is established; it can be difficult to install Stata and may require an individual license; any non-standard R packages need to be installed manually by a UCSB person managing the SCRE 2.6.3 Other best practices Regardless of the data storage option chosen, we recommend several additional best practices: High-level metadata for all datasets should be added to the _emlab_data_directory. See this section in the emLab SOP for further description of the emLab data directory. This will help emLab be internally transparent in how we are using data for different projects, even if the entire group doesn’t have access to the data. For datasets that are confidential, ensure that the “Permissions” column is set to “Secure = confidential data and likely involves a DUA or NDA.” Researchers should consider anonymizing individual-level data before publicly releasing the data (see this R package as one example for how to do this). "],["3-code.html", "3 Code", " 3 Code All data processing and analysis should be performed with code (i.e., avoid spreadsheets), and all code should be packaged in scripts that are version controlled and follow a style guide. Using code and scripts allows for better organization, documentation, and reproducibility of analysis workflows. All emLab code should be stored in the emLab GitHub account. Recommended readings: Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk About Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928. Stodden, Victoria, and Sheila Miguez. 2014. “Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research.” Journal of Open Research Software 2 (1): e21. https://doi.org/10.5334/jors.ay. Wilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745. https://doi.org/10.1371/journal.pbio.1001745. Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510. "],["3.1-scripts-and-version-control.html", "3.1 Scripts and Version Control", " 3.1 Scripts and Version Control Code should be crafted according to the following guidelines: Use scripts Document scripts, but not too much Organize scripts consistently (see format below) Use Git to version control scripts Make atomic Git commits (see description below) Script files should be documented and organized in such a way to enhance readability and comprehension. For example, use a standardized header for general documentation and sections to make it easier to understand and find specific code of interest. Code should also be self-documenting as much as possible. Additionally, use relative filepaths for importing and exporting objects. Scripts should also be modular by focusing on one general task. For example, use one script for cleaning data, another script for visualizing data, etc. A makefile can then be used to document the analysis workflow. There is an art to this organization, so just keep in mind the general principle of making code easy to understand, for your future self and for others. Here is an example template for R scripts: # ============================================================================= # Name: script.R # Description: Visualizes data # # Inputs: data.csv # Outputs: graph.png # # Notes: - Use a diverging color palette for best results # - Output format can be changed as needed # ============================================================================= # Set up environment ---------------------------------------------------------- library(tidyverse) # Set path for Google Drive filestream based on OS type # Note - this will work for Windows and Mac machines. # If you use Linux, you will need to set your own path to where Google Drive filestream lives. team_path &lt;- ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Windows&quot;,&quot;G:/&quot;,&quot;/Volumes/GoogleDrive/&quot;) # Next, set the path for data directory based on whether project is current or archived. # Note that if you use a different Shared Drive file structure than the one recommended in the &quot;File Structure&quot; section, you will need to manually define your data path. # You should always double-check the automatically generated paths in order to ensure they point to the correct directory. # First, set the name of your project project_name &lt;- &quot;my-project&quot; # This will automatically determine if the project exists in the &quot;current-projects&quot; or &quot;archived-projects&quot; Shared Drive folder, and set the appropriate data path accordingly. data_path &lt;- ifelse(dir.exists(paste0(team_path,&quot;Shared drives/emlab/projects/current-projects/&quot;,project_name)), paste0(team_path,&quot;Shared drives/emlab/projects/current-projects/&quot;,project_name,&quot;/data/&quot;), (paste0(team_path,&quot;Shared drives/emlab/projects/archived-projects/&quot;,project_name,&quot;/data/&quot;))) # Import data ----------------------------------------------------------------- # Load data from Shared Drive using appropriate data path my_raw_data &lt;- read_csv(paste0(data_path,&quot;raw/my_raw_data.csv&quot;)) # Process data ---------------------------------------------------------------- # Analyze data ---------------------------------------------------------------- # Visualize results ----------------------------------------------------------- # Save results ---------------------------------------------------------------- Git tracks changes in code line-by-line with the use of commits. Commits should be atomic by documenting single, specific changes in code as opposed to multiple, unrelated changes. Atomic commits can be small or large depending on the change being made, and they enable easier code review and reversion. Git commit messages should be informative and follow a certain style, such as the guide found here. There is also an art to the version control process, so just keep in mind the general principle of making atomic commits. More advanced workflows for using Git and GitHub, such as using pull requests or branches, will vary from project to project. It is important that the members of each project agree to and follow a specific workflow to ensure that collaboration is effective and efficient. "],["3.2-style-guide.html", "3.2 Style Guide", " 3.2 Style Guide At emLab, we recommend using a consistent code style for each of the different programming languages we use. Recommended coding styles for a particular language are collated in what is known as a “style guide”. Style guides typically include standardized ways of naming script files, defining functions and variables, commenting code, etc. While emLab does not mandate the use of style guides, having a consistent code style allows all emLab staff to easily understand each other’s code, collaborate on projects, and jump in on new projects. We consider this an important aspect of collaboration, reproducibility, and transparency. Rather than re-invent the wheel, we leverage existing code style guidelines for each of the languages we use. Below is a summary of the code style guidelines we recommend for various languages: R - Tidyverse Style Guide, by Hadley Wickham Python - Google’s Python Style Guide Stata - Suggestions on Stata programming style, by Nicolas J. Cox Other languages - Google style guides for other languages "],["3.3-reproducibility.html", "3.3 Reproducibility", " 3.3 Reproducibility Prioritizing reproducibility when writing code code not only fosters collaboration with others during a project, but also makes it easier for users in the future (including yourself!) to make changes and rerun analyses as new data become available. Some useful tools and practices include: Commenting code: Adding brief but detailed comments to your scripts that document what your code does and how it works will help others understand and use your scripts. At the top of your scripts, describe the purpose of the code as well as its necessary inputs, required packages, and outputs. File paths: Avoid writing file paths that only work on your computer. Where possible, use relative file paths instead of absolute files paths so your code can be run by different users or operating systems. For R users, using R Projects and/or the here package are great ways to help implement this practice. Functions: If you find yourself copying and pasting similar blocks of code over and over to repeat tasks, turn it into a function! R Markdown: R users can take advantage of R Markdown for a coding format that seamlessly integrates sections of text alongside code chunks. R Markdown also enables you to transform your code and text into report-friendly formats such as PDFs or Word documents. Git and GitHub: As described above, Git tracks changes to files line-by-line in commits that are attributable to each team member working on the project. GitHub then compiles the history of any Git-tracked file online, and synchronizes the work of all collaborators in a central repository. Using Git and GitHub allow multiple people to code collaboratively, examine changes as they occur, and restore prior versions of files if necessary. We also recommend managing package dependencies and creating a reproducible coding pipeline. These two aspects are discussed in more detail below. 3.3.1 Package management A key aspect of reproducibility is package dependency management. For example, your code may run perfectly when using an old version of a particular package, but might fail when using the latest version of that package. In this case, even if someone has the exact code you used, it would fail to run on their machine if they are using the latest version of that package. It is therefore important that along with providing the code you use, you should also somehow provide the packages and package versions you use. If using R, we recommend using renv package for managing package dependencies. renv was developed by the R Studio (now Posit) team. It is used at the project-level, and can thus be included as part of your GitHub respotitory. To set up your project to use renv, first initialize it by running renv::init(). You will then be able “snapshot” the packages and package version you are using in a project (by running renv::snapshot), and save that snapshot so that yourself or others can “restore” to that snapshot and use the exact same packages and package versions (by running renv::restore). This will ensure that your code will run on other machines, even if new versions of packages come out that are not backwards compatible. We also recommend you set up R Studio to use the Posit Public Package Manager instead of the default option of CRAN. The Posit Public Pckage Manager hosts binaries of all of the latest and older historic package versions, which means that renv will always be able to find the correct package version, and should be able to install it more quickly than needing to install it from source which may be required by CRAN. See this link for instructions, and this link for why this is important. 3.3.2 Coding pipelines Often, a project contains multiple script files that are each run separately, but which produce or use interconnected inputs and outputs through a coding pipeline. They must therefore be run in the correct order, and any time a change is made to an upstream script or file, all downstream scripts may need to be run for their outputs to stay up-to-date. This presents a challenge to reproducibility, and also a challenge to anyone trying to work with or review complicated coding pipelines. Additionally, not knowing which files and scripts are up-to-date could mean that long-running operations are re-run unnecessarily. Consider the following example, from Juan Carlos Villaseñor-Derbez’s excellent tutorial: Make: End-to-end reproducibility. Consider you have the following project file structure in a GitHub repository, where clean_data.R is run first and processes the raw data file raw_data.csv. When this is run, clean_data.R produces the cleaned data file clean_data.csv. plot_data.R is run next, which uses clean_data.csv and produces the final output figure1.png. data |_raw_data.csv |_clean_data.csv scripts |_clean_data.R |_plot_data.R results |_figure1.png |_raw_data.csv |_clean_data.csv These scripts must be run in the correct order, and any time changes are made to upstream data or scripts, downstream scripts must be run to update downstream data and results. To ensure reproducibility of your code, we recommend everyone employ one of the three following options when using a coding pipeline, with the four options in ascending order of recommendation from “minimum required” to “best practice.” We provide a best practice for non-R code, as well as a best practice for R code. Option 1 (minimum required): Provide clear documentation in the repo’s README.md file. This documentation should provide the complete file structure, including both scripts and data files, and should provide a narrative description of the order in which the scripts should be run, as well as which files each script uses and produces. Option 2 (better practice): Create a run_all.R script that runs all scripts in the correct order, and can be run to fully reproduce the entire project repo’s outputs. This run_all.R script should be described in README.md. Using the example above, this script could contain the following: # Run all scripts # This script runs all the code in my project, from scratch source(&quot;scripts/clean_data.R&quot;) # To clean the data source(&quot;scripts/plot_data.R&quot;) # To plot the data Option 3 (best practice for non-R code): For non-R code, we recommend using the make package. This uses a Makefile to fully describe the coding pipeline through a standardized syntax of targets (output files that need to be created), prerequisites (file dependencies), and commands (how to go from prerequisites to targets). The advantage of this approach is that it automatically runs scripts in the correct order, keeps track of when changes are made to scripts or data files, and it only runs the necessary scripts to ensure that all outputs are up-to-date. If using make, you should describe how to use it in the repo’s README.md. The Makefile uses the following syntax: target: prerequisites command Therefore, in our example, the Makefile would simply be the following, and the single command make would execute this: results/figure1.png: scripts/plot_data.R data/clean_data.csv Rscript scripts/plot_data.R data/clean_data.csv: scripts/clean_data.R data/raw_data.csv Rscript scripts/clean_data.R Please refer to Juan Carlos Villaseñor-Derbez’s very helpful tutorial for more information: Make: End-to-end reproducibility. Option 4 (best practice for R code): For R code, we recommend using the targets package. Targets is a Make-like pipeline tool that has been specifically developed for R. Using targets means that anytime an upstream change is made to the data or models, all downstream components of the data processing and analysis will be re-run automatically when the targets::tar_make() command is run. It also means that once components of the analysis have already been run and are up-to-date, they will not need to be re-run. All interim objects are cached in a _targets directory. This directory can either be within in the project directory when the objects are small (e.g., within the GitHub repo), or can be set to another directory when objects are larger (e.g., on Google Shared Drive or Google Cloud Storage). Targets also has a built-in parallel processing option, which can allow long-running components of the analysis to be run in parallel. It also plays nicely with renv for package management. For further information, the targets package documentation is a great place to start. We also recommend the presentation (and associated GitHub repository) developed by Tracey Mangin for R-Ladies Santa Barbara as another great primer. "],["3.4-internal-code-review.html", "3.4 Internal code review", " 3.4 Internal code review Peer-review of each other’s code is a great method for mutual learning through exposure to different coding styles and packages, ensuring reproducibility, catching any bugs, and uncovering opportunities for doing things better. We recognize that this takes time, but we believe that the time commitment is worth it for us to all become better coders and write better code. As a best practice, we recommend that for any project that has at least two researchers, there should be a systematic review of any major coding elements by the researcher who did not initially write the code. By following the best practices outlined in the Coding Pipeline section of this SOP, the original coder can ensure that the reviewing coder knows exactly which pieces of code are critical for their review. Code reviews should be respectful and collegial in manner. The goal is to make all of our work better and make all us better coders, not to criticize or single anyone out. The original coder and reviewing coder should coordinate on the best method for feedback, whether that is through direct pull requests to the code, an email outlining suggestions, or a Google doc outlining suggestions. For further guidance on effective code reviewing techniques and guiding principles, we recommend using the Tidyteam code review principles. These were developed by the tidyteam for maintaining packages such as those found in the tidyverse and tidymodels. These guidelines are based on Google’s Code Review Developer Guide, another helpful resource. "],["3.5-exploratory-data-analysis.html", "3.5 Exploratory Data Analysis", " 3.5 Exploratory Data Analysis Much of the work emLab does is Exploratory Data Analysis (EDA), especially during the beginning stages of a project when we are familiarizing ourselves we new datasets and rapidly prototyping data wrangling and modeling approaches. The following quote from Hadley Wickham’s R for Data Science describes EDA as: EDA is an iterative cycle. You: 1) Generate questions about your data; 2) Search for answers by visualising, transforming, and modelling your data; 3 Use what you learn to refine your questions and/or generate new questions. EDA is a creative and exploratory process without defined rules. Hadley Wickham’s chapter on EDA is an excellent place to start for ideas, but we encourage creativity and exploration during this phase of a project. However, we recommend as a best practice that all EDA should include an “Interpretations, questions, and new ideas” section. The researcher doing EDA is poised in an excellent position for providing initial interpretations of the data, raising outstanding questions about the data, and generating novel insights and potential research questions. This section should contain the following information: Interpretations about what you found. Does it match our intuition? Is there anything surprising? Questions about the data. Are there any questions about how to use or interpret the data? Are there any potential problems with the data or analysis? New insights and research questions. Have you uncovered any new insights about the data or research? Have you discovered any new research questions that might be worth pursuing? By including a section in each EDA analysis that discusses these aspects, we can build insight generation into our workflows. This information should be in an easy to find location on the Team Drive, such as in a markdown-reports directory that contains EDA PDFs generated by R Markdown. This way, other members of the project team can read it, digest it, and respond with further ideas. "],["4-high-performance-computing.html", "4 High Performance Computing", " 4 High Performance Computing Certain analysis use cases require high performance computing resources: big data parallel computing lengthy computation times restricted-use data For analyses involving big data or models that take a long time to estimate, a single laptop or desktop computer is often not powerful enough or becomes inconvenient to use. Additionally, for analyses involving restricted-use data, such as datasets containing personally identifiable information, data use agreements typically stipulate that the data should be stored and analyzed in a secure manner. In these cases, you should use the high performance computing resources available to emLab. emLab currently has two high performance computing servers that are managed by UCSB’s General Research IT (GRIT). These servers are named sequoia and quebracho. This section of the manual describes how to use these two servers for high performance computing. For now, please use sequoia for general emLab computing for most projects. Quebracho is currently restricted to land use projects (e.g., land-based-solutions and projects starting with cel), so please only use quebracho if you have already been doing so and have already discussed this with Kathy or Robert. If you have any doubts about which server to use, please use sequoia. Note that sequoia does not have a GPU, but quebracho does. If you need access to a GPU and are not already using quebracho, please contact Robert and Kathy to talk about using quebracho. "],["4.1-available-resources.html", "4.1 Available resources", " 4.1 Available resources HPC resources available to emLab researchers Cores RAM GPU USE quebracho 64 1TB Yes Only land-use projects (please check before using) sequoia 192 1.5TB No All other emLab research Knot 1,500 48 GB - 1TB Yes UCSB shared resource Pod ~2,600 190 GB - 1.5 TB Yes UCSB shared resource Braid2 ~2,200 192 - 368 GB Yes UCSB condo cluster (PI must buy node) The emLab SOP will focus on using quebracho and sequoia. For further information on using other UCSB campus resources, you can refer to our specific guide on that. However please note that this guide is several years out-of-date, and you may find better and more current information directly on a UCSB website. Additionally, now that we have our own HPC servers, we no longer recommend using Google Compute Engine, which is a pay-as-you-go cloud computing server. It can be quite expensive, and has setup challenges as compared to our own servers. However, if you need to use GCE for whatever reason, emLab alumni Grant McDermott wrote a very helpful tutorial on using R Studio Server on GCE. "],["4.2-available-software.html", "4.2 Available software", " 4.2 Available software Both quebracho and sequoia currently have R Studio Server, Jupyter Notebook, and VSCode installed. Positron is available on sequoia through a remote desktop Linux server. GRIT manages these installations for us. They will also manage updates for these. Both quebracho and sequoia also leverage SLURM queueing systems. All computational activity is forced to go through SLURM, even interactive R Studio and Jupyter Notebook sessions (see section on Open OnDemand below). Scrontab can be used to manage SLURM crontab files. If we wish to install additional software, we will need to decide on these as a group and have GRIT install them for us. When considering new software to install, we should consider whether or not it is already available on other campus servers; what it will cost; and how many people in emLab would use it. Generally speaking, if a specific piece of software is expensive (e.g., Stata or Matlab), will not be used by too many emLab folks, and is already available on other campus servers, we should rely on these other campus servers and not install it on our own servers. Users interested in MatLab should first try Pod which has the necessary licenses and is available for free. If users wish to use python it is recommended that they install Visual Studio Code (VS Code) available for free from Microsoft. With VS Code installed, users can add the Remote SSH extension and access sequoia via SSH tunnel. Further instructions can be found in the VS Code Documentation. After accessing sequoia via SSH tunnel, users may install their preferred python distribution. Miniconda is a good starting point, though other options are available. This Medium article is a good place for further installation guidance. Finally, it is recommended to create custom python environments for each project. All ssh sessions go into the slurm queue. For users that need Stata, it is already available on both UCSB’s Knot cluster. More details for using Stata on Knot can be found here. We will not be installing Stata on quebracho or sequoia. For users of Matlab, it is already available on all campus clusters. More details can be found hehttps://csc.cnsi.ucsb.edu/docs/using-matlabre. We will not be installing Matlab on quebracho or sequoia. "],["4.3-installing-packages.html", "4.3 Installing packages", " 4.3 Installing packages You can install regular user-level R packages just like you would normally using R Studio on your local machine. We recommend using the renv R package to manage package dependencies for each project (i.e., GitHub repo) you work in. Please refer to the emLab SOP section on reproducibility for more information on renv. Additionally, GRIT installs and updates many commonly used R packages on the servers, which are accessible in a “site-library” for each server. They update these once or twice a year. To add the GRIT R package library to your library paths, you can run this line of code: .libPaths(c(\"/usr/local/lib/R/site-library/\", .libPaths())) For system-level packages that you would normally need to install through the terminal on your local machine (e.g., packages like gdal or libproj), we will need to have GRIT install and manage these for us. We have already had GRIT install many commonly-needed system-level packages, which they will update once or twice a year. If you need a particular package that is not yet installed, please start a help ticket directly with GRIT: help@grit.ucsb.edu . "],["4.4-setting-up-a-grit-account.html", "4.4 Setting up a GRIT account", " 4.4 Setting up a GRIT account To use emLab’s HPC servers, you must have a GRIT account. Please refer to the emLab Manual section on setting up and managing an account with GRIT. "],["4.5-using-sequoia.html", "4.5 Using sequoia", " 4.5 Using sequoia Sequoia leverages Open OnDemand (OOD), a system managed by GRIT to connect our HPC resources to commonly used software. OOD is accessed via an online dashboard in your web browser: https://hpc.grit.ucsb.edu Once at the website, you can use your GRIT user ID and password. The system does not require you to be on campus or use a VPN. Through OOD, you can use the following software: R Studio Server Jupyter Notebook VS Code Server Positron (via a Linux remote desktop server) To use any of these, click “Interactive Apps”, then click on your software of choice (to access Positron, click “Desktop”). Each time you launch an interactive app, you need to specify the following up-front before launching the job: Partition name: Type emlab_nodes to use emLab’s private sequoia resources (this should be used in most cases). Alternatively, you can type grit_nodes to use GRIT’s campus-wide shared resources Job duration (up to 168 hours): Your interactive session will run for this amount of time, and then shut down (you can cancel jobs before the ending time if you desire) Number of cores: How many CPU cores you want to use for your interactive session. Currently, we have this set to a maximum of 24 cores per session. Since sequoia is a shared resource, please be considerate of others when requesting cores. If you are unsure of how many cores to use, we have another section of the SOP below to help you figure this out. RAM: How much RAM you want to use for your interactive session. Currently, we have this set to a maximum of 256GB per session. Since sequoia is a shared resource, please be considerate of others when requesting cores. If you are unsure of how much RAM to use, we have another section of the SOP below to help you figure this out. Please do not enable to “Use GPU” option unless you have already discussed this with Kathy or Robert (it is only available on quebracho for land use projects at this time). Note that for convenience, your last used settings will be saved for the next time you launch an interactive app. Once you’ve configured your session, click “Launch”. It may take a few minutes for your session to start up. Once it is ready, you will see a link to open R Studio Server (or whichever app you selected). Click the link to open R Studio Server in a new browser tab. Note that you may use OOD to launch multiple interactive sessions at the same time! They are each still managed through SLURM. More resources for OOD are available on GRIT’s bookstack: OOD landing page R Studio Server Jupyter Notebook VS Code Server "],["4.6-best-practices.html", "4.6 Best practices", " 4.6 Best practices Here we outline our best practices for using shared computational resources. These are meant to be living guidelines that will be adapted by our team as needed: Sharing is caring! Common courtesy can go a long way. As much as possible, try to use only the resources you need. Leverage the tools below to monitor how many cores and how much RAM you and others are currently using In general on sequoia, feel free to run analyses that use up to 24 cores and 256GB of RAM. We will likely adaptively manage these specific numbers once we start using sequoia and getting a better understanding of how many resources we are using. And if you don’t need that much, please request less so that others can use the resources they need. For larger analyses that require lots of cores or RAM, coordinate with others over the server slack channel (#hpc-core-dination) to ensure that workflows are not disrupted and that everyone has reasonable access to computational resources Generally, we recommend piloting your code using a small subset of your data and/or just a single core, either on your local computer or on one of our HPC servers. Then once you know it works and have a sense of how much memory it will use and how long it will take to execute, you can go ahead and run the full analysis on the server. And if it looks like the full analysis will require resources beyond the standard recommend 24 cores and 256GB, coordinate with the team on the Slack channel #hpc-core-dination. "],["4.7-resource-allocation.html", "4.7 Resource allocation", " 4.7 Resource allocation It is up to each researcher to: 1) decide how much computational resources they need prior to starting each job (i.e., how many cores and how much RAM will your job need); and 2) monitor their resource usage during each job to not only ensure they are not exceeding their allocated resources, but to help build personal awareness on how much resources certain jobs require. It will generally take each person some time to develop an intuition on how many resources each job will need; we always recommend piloting code locally on your own machine to start getting a sense for this. Additionally, as you use the server more and the tools below, you will further develop this intuition. Available tools: Zabbix dashboard: GRIT manages a dashboard for us which provides a high-level system-level view of how many resources are being used on each of our servers. This is a good place to start to get a sense of overall resource usage across the whole team. It is also a good place to check before starting a new job to see how busy the servers are. It is also a place that will show the current status of each server and whether or not there are currently any problems. Job Resource Utilization Analyzer: If using sequoia via OOD, you can navigate to the OOD dashboard, click “Apps”, then click “Job Resource Utilization Analyzer”. For each job that is currently running on a GRIT server, this will show you how many cores and how much RAM was requested, as well as the actual peak core and RAM usage. It further provides some information on whether or not you over-requested core and/or RAM resources, and provides some recommendations if any adjustments should be made in the future. This is a nice way to monitor live usage. Post-job email: If using sequoia via OOD, at the end of each job you will receive an email summary of your job resource usage. This email provides similar information to the Job Resource Utilization Analyzer, but is sent to you automatically at the end of each job. It also provides information on whether or not you over-requested core and/or RAM resources, and provides some recommendations if any adjustments should be made in the future. This is a nice way to monitor your resource usage after the fact. GRIT provides some helpful rules of thumb: “If Max RSS is much lower than requested memory, you may be over-requesting RAM.” “If the job failed with OUT_OF_MEMORY and Max RSS is close to the request, you likely need to request more memory.” “If Total CPU &lt;&lt; (Elapsed time * AllocCPUS), your job may be I/O bound or under-utilizing its cores.” htop: This terminal tool is installed on each server and provides a real-time view of resource usage on each server. It is a great tool to use during an interactive session to see how many cores and how much RAM your job is using, and also how many cores and how much RAM are currently being used by others. You can customize the htop display to make things easier to see. For example: After entering htop, press F2 to enter setup. You can also click directly on setup to enter it. Once you enter setup, if you have trouble seeing the setup options, you can try reducing your browser’s text size temporarily in order to see the setup options. Sequoia has 192 cores so the default view with 4 columns means a pretty large display. In the Meters setup, you can change the left column to be CPUs (1-4/8) [Bar] and the right column to be CPUs (5-8/8) [Bar] This will condense the output and force 8 columns.  I also like to add disc IO to the left column below memory.  In the “Display options” setup you can select some that will clean up the process information below the resources monitor. I like to make sure to select  Tree view Tree view sorted by PID Shadow other users’ process (makes it easier to see your own) Count CPUs from 1 Enable the mouse Press F10 when done "],["4.8-using-quebracho.html", "4.8 Using quebracho", " 4.8 Using quebracho On your local machine, connect to the UCSB Campus VPN. You can do so by downloading a VPN client for your operating system, such as Ivanti Pulse Secure. More details for connecting to the UCSB VPN and installation instructions are provided here. Note: Even if you are on the UCSB campus you will still need to connect to the campus VPN. Once you’ve connected to the VPN, you are ready to access the server. You can access quebracho via SSH if you wish to use something like VS Code. To access R Studio Server, you can simply navigate to the following link. Once there, you will be prompted to enter your GRIT user ID and password. Once you’ve done this, you are ready to use R Studio Server! Quebracho: https://quebracho.geog.ucsb.edu/rstudio/ "],["4.9-accessing-data.html", "4.9 Accessing data", " 4.9 Accessing data Please refer to this section of the emLab SOP for a description of the data directory structure for our emLab GRIT data storage space. All data in the emLab GRIT data storage space can be directly accessed on each of the servers (sequoia and quebracho) without any changes to the directory paths. All data in the emlab/data and emlab/projects/current-projects directory physically lives on high-speed hard drives attached to sequoia, so if you need to work on data in these directories, you will have the best computing performance when using sequoia. Please refer to this section of the emLab SOP for a code snippet that can be used to directly access data on the server in R. In addition to having access to our emLab GRIT data storage space, which is shared across all members of our team, all individual users also have a private user-specific storage space. All GRIT users get a free 50GB personal storage space by default. As a general best practice, we recommend storing all data on the emLab data storage space, and only storing cloned GitHub repos and user-specific R packages and settings in your personal user space. For example, you should store all project-specific data in the appropriate directory under the emLab data storage space, but you should store all of your cloned GitHub repos s in your personal storage space. By default, when you clone repos from GitHub they are stored in your personal storage space, along with any of your user-specific R packages and configurations. If for whatever reason your personal storage space exceeds 50GB, it will stop working, so you should ensure you always have a safe buffer. However, we envision that if users only keep cloned GitHub repos and R packages in their personal user space, they should not need to worry about hitting the 50GB limit. You can check your current personal storage by typing df -h in the terminal and then looking for your username. "],["4.10-accessing-code.html", "4.10 Accessing code", " 4.10 Accessing code Here we can talk about how to use the servers with GitHub code management. Essentially, you can work with projects and GitHub repositories on RStudio Server exactly like you can on your personal machine. One major difference is how to set up GitHub authentication, which is a little different on the servers than it would be on your personal machine. So we can provide explicit instructions on doing that. Please refer to this section of the emLab SOP for directions on how to set up and manage git and GitHub for your new server workspace. One important difference between your personal laptop and using a server is that file permissions may be such that other users can see and sometimes read or write files in your directories. Ideally, any confidential information such as your git credentials should be secured differently from your personal computer. Step 6 of the Git and Github section of the emLab manual is therefore not recommended in a multi-user server environment because your token may end up viewable to other users as plain text.  Instead of storing your Personal Authentication Token (PAT) as plain text, it is recommended to use one of the following options. Using either of these two approaches will also mean that credentials are stored between sessions, which should make the user experience a bit easier. The first approach, using an SSH key, is recommended Use an SSH key instead of a PAT Set up your SSH key on the GRIT server. If you are not using R Studio Server, or prefer to use the terminal, follow these instructions:  You can generate a new SSH key with the terminal command ssh-keygen -t ed25519 -C “email@example.com” You are prompted to select a location (hit enter for the default location) You are prompted to set a password (hit enter to not require one) Start your SSH agent in the background with  eval “$(ssh-agent -s)” Add your private key to the SSH agent with ssh-add ~/.ssh/id_ed25519 Copy your public key with  cat ~/.ssh/id_rsa.pub | xclip -selection clipboard If you are using R Studio Server and you prefer to not use the terminal approach, the instructions are a bit more streamlined. Follow the instructions in this link.   If after going through these instructions you prefer to not use a password, you can remove it using the instructions provided in this link. Add your public key to your GitHub account GitHub &gt; Settings &gt; SSH and GPG keys &gt; New SSH key Paste in your key (either from Step 6 in the terminal option above, or copied from R Studio Server in the R Studio option above) Now you can clone your repository onto the GRIT server. This means that you need to either: 1) when cloning the repository for the first time you need to use the SSH url rather than the HTTPS url; or 2) if you’ve already clone the repository, set your repository URL to the SSH version If cloning a repo for the first time using R Studio Server, you can simply click “File &gt; New Project &gt; Version Control &gt; Git”, and then enter your repo’s SSH link git@github.com:username/example_repo.git (for example, this might look like git@github.com:emlab-ucsb/ocean-ghg.git) Alternatively, in the terminal You can manually set specific repo URLs to SSH with:  git remote set-url origin git@github.com:username/example_repo.git (for example, this might look like git@github.com:emlab-ucsb/ocean-ghg.git) Caching your PAT temporarily  Create a PAT on GitHub Add credential cache timeout instructions to your git config file git config –global credential.helper ‘cache –timeout=3600’ Adjust the timeout length (in seconds) as needed  Push changes to GitHub When prompted enter your username  For the password enter your PAT Future pushes will not not require you to enter credentials within the timeout window This is not a good long term solution because you will need to re-enter your credentials anytime the server restarts or when your cache timeout ends "],["5-reports-and-publications.html", "5 Reports and Publications", " 5 Reports and Publications This section describes best practices related to emLab reports and publications. "],["5.1-emlab-affiliation.html", "5.1 emLab Affiliation", " 5.1 emLab Affiliation Since emLab is joint between MSI and Bren and also its own research entity, we recommend emLab staff use the following affiliations for any emLab publication: Marine Science Institute, University of California, Santa Barbara, Santa Barbara, CA, USA Bren School of Environmental Science &amp; Management, University of California, Santa Barbara, Santa Barbara, CA, USA Environmental Markets Lab, University of California, Santa Barbara, Santa Barbara, CA, USA Note that journals may list affiliation addresses slightly different than what is listed above. "],["5.2-reports.html", "5.2 Reports", " 5.2 Reports Many final deliverables for our projects include creating reports. To aid in the final report development, please see the Report template for information on a draft cover page, inside page, logos to use, and suggested citation. To ensure future searchability, please upload all final reports to Zenodo - a public repository that creates a DOI and link for all uploaded information - if the report can be made publicly available. If you don’t have an account, you can sign up here with your email, GitHub account, or ORCID. We have created an emLab repository for all of our reports to live. To upload to the emLab community repository, use this link and fill out all of the required information. "],["5.3-author-contribution.html", "5.3 Author Contribution", " 5.3 Author Contribution Transparency in authors’ contribution is a critical component of open science. Determining authorship is ultimately the responsibility of the project leads (i.e. principal investigator and/or first author), but because author contribution is not always straightforward, we follow McNutt et al. (2018) and others in recommending the Contributor Roles Taxonomy (CRediT) system. For a simplified version of this most relevant to our project infrastructure, feel free to use and modify our Author Contribution Template, which allows individuals to identify their contributions or to opt-out of participation in a paper. We recommend discussing authorship expectations early in a project to avoid future complications. "],["5.4-making-your-data-publicly-available.html", "5.4 Making Your Data Publicly Available", " 5.4 Making Your Data Publicly Available Generally, emLab researchers should be prepared to share a public GitHub repository (see Section 6.4) and a Dryad data repository with publications. The repositories contain code scripts and data (respectively) needed to conduct the study. Increasingly, journals are requiring both sets of documentation at the point of submission or publication. Dryad is an open-access repository of research data that makes data searchable, freely accessible, and citable. Data repositories do not need to be associated with a publication. Dryad is free for UCSB affiliates with a UCSB NetID. The following steps document the process for creating a Dryad repository. Review Dryad’s Best Practices website for more useful information. If you do not already have one, visit the ORCID site to obtain an ORCID identifier. You will need this to deposit data on Dryad. Use your ORCID username and password to sign into Dryad. Under My Datasets, select “+ Start New Dataset.” Fill in the required fields and describe your dataset. If your data is related to a manuscript in progress or a published article, you will be asked to provide the Manuscript Number or DOI. Upload your files. Note that while you cannot load a folder to Dryad, you can load a .zip file that includes a folder structure. This approach is suggested as it allows for improved organization. Review and submit. In this phase you can choose to “Enable Private for Peer Review,” which keeps your dataset private during your article’s review period. You will have access to a private dataset download URL that you can share with collaborators or the journal. If you make this selection, your dataset will not enter curation or be published. If you do not select “Enable Private for Peer Review,” you will submit your dataset to Dryad for curation. Skip to step 9 if you did not “Enable Private for Peer Review.” If you chose “Enable Private for Peer Review,” copy the URL to share with collaborators or the journal. Those who use this link may be asked to provide an email address in order to obtain the dataset. Note that in this case, users will receive an email from Dryad with instructions for how to download the dataset – notify users to check their Spam folder for this email. Once the manuscript is accepted, you can go back to your dataset and submit it to Dryad for curation. Make sure to incorporate any relevant changes that occurred during the revision period. During curation, Dryad will check your submission to ensure the validity of files and metadata. Dryad may contact you questions, suggestions, and/or identified problems. Once the dataset is approved, the Dryad DOI is officially registered and made public. Note that you can contact Dryad in order to delay the publication of your data until your publication date. Include DOI in your manuscript’s Data Availability statement, or consider citing it the References section. "],["5.5-preparing-a-public-github-repository.html", "5.5 Preparing a Public GitHub Repository", " 5.5 Preparing a Public GitHub Repository As with data, we strive to make all our code available. This provides a roadmap of converting input data into tangible results, which may be of interest for external people seeking to replicate our study or for internal emLabers seeking to understand what someone did a few years ago. 5.5.1 Documentation One of the most important things to include in the repo is a README.md file. This will be automatically displayed as rendered markdown on GitHub, and should provide a simple explanation of what’s in the repo, how to run it, and how it was run in the past. If possible / necessary, you might want to include a file structure (Take a look at using startR::create_readme() for automating this). If relevant, you might want to include the title of the paper / project, and a link to any online material (e.g. the publication itself). In paragraph or bullet-list form, make sure to specify the following: Operative System(s) in which the project was run (e.g. MacOSX Catalina or Ubuntu 18.4) The version of R / STATA / MATLAB / Julia / Python… including major and minor (e.g. R 3.6.2) Any special mentions of performance needed (e.g. “This analyses requires a machine with at least 32 GB RAM and 16 cores”) Link to any relevant data repositories Any relevant contact information, should interested people have trouble running your code When in doubt, check out the repository that Grant McDermott and Matt Burgess provided for their Science paper on Effort reduction and bycatch. 5.5.2 Sanitizing the repository When tracking a project, we’ll usually end up with many small, meaningless commit messages such as “fixed typo”, “fixed bug”, or “actually fixed bug”. While these small incremental changes allow us to revert back during the production process, in the end, we may not want to have the full list of bug fixes and meaningless commit messages visible. Thankfully, Git allows us to clean things up a bit using git rebase. Here’s an example of what your code might look like: 871adf OK, plot actually done --- newer commit 0c3317 Whoops, not yet... 87871a Plot finalized afb581 Fix this and that 4e9baa Fixed typo on x-axis d94e78 Plot model output 6394dc Fixing model --- older commit The top 6 commit messages are all related to each other. And, had you been making this plot at 9 am and not 3 pm, it would all have been a single push. Instead, we might want this to look like this: 871adf OK, plot actually done --- newer commit -┐ 0c3317 Whoops, not yet... | 87871a Plot finalized | afb581 Fix this and that | ---- Join all this into one 4e9baa Fixed typo on x-axis | d94e78 Plot model output -------------------┘ 6394dc Fixing model --- older commit In this case, we want to merge the last 6 commits into one. We want it to look like this in the end: 84d1f8 Plot model output --- newer commit (result of rebase, combining 6 messages) 6394dc Fixing model --- older commit We can do so by running the following line: Notice that I’ve specified the value 6 after the argument HEAD~. If you don’t want to count the number of commits, you can simply reference the last commit (by its hash) that you want to leave out. For example, we wanted to leave out the Fixing model commit, with hash (6394dc). Therefore, we can also run: Whichever way you go, your predetermined text editor will open. You’ll see a list of commits, containing the ones you want. (Head’s up, the older one will be on top). At the bottom of the page, you’ll see the following list of possible instructions: # Commands: # p, pick &lt;commit&gt; = use commit # r, reword &lt;commit&gt; = use commit, but edit the commit message # e, edit &lt;commit&gt; = use commit, but stop for amending # s, squash &lt;commit&gt; = use commit, but meld into previous commit # f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit&#39;s log message # x, exec &lt;command&gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with &#39;git rebase --continue&#39;) # d, drop &lt;commit&gt; = remove commit # l, label &lt;label&gt; = label current HEAD with a name # t, reset &lt;label&gt; = reset HEAD to a label # m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;] # . create a merge commit using the original merge commit&#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c &lt;commit&gt; to reword the commit message. You’ll need to preface the hash with whichever command (or shortcut) you want to use. You might want to reword a commit (i.e. remove all those “F%@#!!”), so you’ll use r. You might want to pick the head of the commit, so you’ll use p. You might want to squash multiple commits into one, so you’ll use s. In the example above, you’ll have to edit the first word of each hash to make it look like this: pick d94e78 Plot model output --- older commit s 4e9baa Fixed typo on x-axis s afb581 Fix this and that s 87871a Plot finalized s 0c3317 Whoops, not yet... s 871adf OK, plot actually done --- newer commit Now, simply save and close the file; you’ll be prompted back to your command line. The next thing to do is to give the new commit a name. Your editor will pup up. You can use the default message, or replace it with something like “Plot model output”. Save the file, close it, and push your changes. You can read much more about this on Git’s help page for Rewriting History (sounds cool, right?). "],["5.6-sharing-public-data-shiny-apps-and-tools-on-our-website.html", "5.6 Sharing public data, Shiny apps, and tools on our website", " 5.6 Sharing public data, Shiny apps, and tools on our website Any time emLab develops publicly available datasets, Shiny apps, or tools, we would love to share these on the emLab website. Any time you release a publicly available dataset, Shiny app, or other tool, please Slack or email Erin O’Reilly (eoreilly@ucsb.edu) the following information: name of the dataset app or tool, link, date published, and the emLab project it is associated with. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
