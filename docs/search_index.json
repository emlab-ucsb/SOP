[["index.html", "emLab Standard Operating Procedures Overview", " emLab Standard Operating Procedures Environmental Markets Lab (emLab) 2023-09-05 Overview This reference guide describes standard operating procedures (SOP) for emLab projects. This SOP is organized into 6 different sections: (1) Project Management, (2) File Structure, (3) Data, (4) Code, (5) High Performance Computing, and (6) Reports and Publications. Section 1 provides an overview of the platforms we use and how to install them; Section 2 covers file structure and storage; Section 3 describes file naming and metadata standards, as well as what the emLab data directory is composed of; Section 4 highlights best practices for coding; Section 5 covers high performance computing; and Section 6 outlines aspects of the publication process from emLab affiliations to preparing a public GitHub repository. "],["1-project-management.html", "1 Project Management", " 1 Project Management This section details the project management platforms we use at emLab to help set up projects for success. The first step for a new project will be to determine a common name to use across these platforms (note: name should be all lowercase with a dash separating words). Please additionally review the project management best practices document. "],["1.1-google-calendar.html", "1.1 Google Calendar", " 1.1 Google Calendar Our team relies heavily on Google Calendar to check team member’s availability and schedule meetings. Please keep your calendar up to date! 1.1.1 Setting Out of Office Notifications The best way to let people know if you are on vacation or out of the office is by setting up your calendar event as “Out of Office” instead of a regular event. You can customize this to automatically decline meetings on that day. 1.1.2 Adding Other Calendars To view team member’s calendars, click on the plus symbol next to “Other calendars” on the left hand side of your calendar and select “Subscribe to calendar.” There is a full list of team emails in the emLab Team Roster document. 1.1.3 Scheduling Rooms See this Room Scheduling document for full details on how to reserve rooms in both Bren and MSI. As a reminder, please add bren-sfg@ucsb.edu to the calendar invite for any meeting held in MSI 1304. "],["1.2-zoom.html", "1.2 Zoom", " 1.2 Zoom Through UCSB, we have access to a paid Zoom account, which we use for all video conferencing. You can schedule a Zoom meeting (or recurring meetings) through your browser or the Zoom app. Below are our Zoom best practices and guidelines. 1.2.1 Making Scheduling Easier To make scheduling with Zoom easier, install Zoom for GSuite. Once synced with your Zoom account, it will show up as an option along with Hangouts under conferencing when you create a calendar invite. Once created, you will also be able to see the meeting on your Zoom account. 1.2.2 Being an Effective Communicator on Zoom… …for meetings Here are a couple of tips for getting the most out of virtual meetings: Everyone on video: it’s nice to see everyone’s face instead of talking to black boxes. If your internet isn’t acting up, turn your video on to have a more engaging conversation. Sharing screens: to keep everyone on the same page, have the presenter share their screen so all of the participants are looking at the same information to avoid any possible confusion. This is a good way to troubleshoot code, walk through documents, etc. The new Zoom default is to only allow the host to screen share. To allow all participants to share their screens, go to your settings and under “who can share?” select “all participants.” Note: If you aren’t comfortable changing this setting, instead turn on the ability to have co-hosts and in the meeting select the screen-sharers to be co-hosts. Whiteboard: Zoom has a whiteboard feature that allows you to draw on your screen for everyone to see. To use this feature, share your screen and select “whiteboard” instead of a browser window. Nonverbal communication: use the reactions feature in the bottom bar of your zoom meeting to react during a meeting (so far there is a clap and thumbs up option). You can also raise your hand in the meeting to signal that you have a question. To raise your hand, open the participant panel by clicking on participants in the lower bar and then click “raise hand” in the lower right hand corner (note: the host can’t raise their hand). Making the most of virtual meetings: Circulate as much as possible (agenda, slides, google docs, etc.) before the meeting so people have time to read/review them If needed, schedule longer meetings to avoid feeling rushed and allow time to repeat things or deal with technical difficulties Notify people ASAP if they are cutting out Dial-in and use phone for audio if your internet connection is poor Turn off videos if connection is poor …for lectures For teaching online courses, here are a couple additional tips. These are more geared towards keeping students engaged in a remote learning setting. Zoom polls: you can enable “polling” for meetings and develop a list of preset questions to ask the group. For example, after introducing a topic, you can have students do a practice problem and respond to the poll with their answer. Instructions on how to do this here. Zoom whiteboard: if you are used to drawing on the whiteboard in class, there is a virtual option for that through zoom! When sharing your screen, select whiteboard instead of your browser and draw away. Breakout rooms: if your classes are more discussion-based, you can use the breakout room feature to break the class into smaller groups. The groups can either be automatically assigned randomly or manually assigned in specific groups ahead of time. More information on breakout rooms and pre-assigning rooms. Record video: record lectures and post them online for students to refer to later. You could also record yourself walking through a homework or test problem for them to reference. Pause for questions: since it will be harder to see students raise their hands, build in more breaks to pause for questions. You can also encourage students to either (1) add questions to the chat and pause once a couple of questions come in or (2) have them use the raise hand feature (see nonverbal communication above). 1.2.3 Avoiding Zoom Bombings UCSB has developed guidelines for preventing Zoom bombing. emLab is not mandating any of these settings, these are just UCSB recommendations for making meetings more secure. Class-specific recommendations here General information on securing Zoom here "],["1.3-slack.html", "1.3 Slack", " 1.3 Slack Our team uses Slack on a daily basis to keep communication channels open within teams and across our different offices. We have an emLab workspace that houses all of our channels. When you are first added to Slack, you will be added to the General, Random, Communications, Report and Publications, and Code channels. We also have a channel for every project, which you will be added to as needed. You can view Slack either through a broswer window or by downloading the desktop app. 1.3.1 Slack Basics Slack is organized into channels and direct messages. Channels are a way to organize conversations and other than a couple general emLab channels, are often project specific. Whatever you share in a channel is viewable by all members of that channel. You can also send direct messages to an individual or a group of up to 9 people. One great thing about Slack is that it’s searchable. You can search either by person or keywords to find old messages. Additionally, if someone sends you something you will need to reference multiple times, you can star messages and view them by clicking the star in the upper right-hand corner. People have differing notification preferences, which you can set under Preferences → Notifications. If someone sends you a direct message or tags you, a number will show up on your slack app. If they add something to a channel you are on but don’t tag you, a red dot will show up. To ensure someone gets a numbered notification, either tag them (i.e. @Erin) or tag the channel (i.e. @channel). Tagging the channel will send a notification to every member of that channel. 1.3.2 Creating a Channel When a new project starts, create a slack channel for it and add the relevant team members to it. To create a channel, simply click on the plus symbol next to channels and fill out the channel information (Name, Purpose, and Send invites to). "],["1.4-airtable.html", "1.4 Airtable", " 1.4 Airtable At emLab, we use Airtable as our primary project management software for tracking project deliverables, activities, and roles. To put it simply, it’s a glorified spreadsheet that you can mold to fit your needs. Email Erin (eoreilly@ucsb.edu) to be added to Airtable. 1.4.1 Helpful Terminology Base: think of it like a database. A base is made up of tabs of spreadsheets that can be customized and linked to one another. Workspace: multiple bases can be organized within a workspace. For example, we have an emLab workspace and individual project bases. 1.4.2 Getting to Know the emLab Workspace Within the emLab workspace, we have the following bases: emLab Projects and Pipeline Project Template Project-specific bases The emLab Projects and Pipeline base contains overview information about current and potential projects. It is organized into the following tabs: Team Directory, Projects (current and archived), Deliverables (as outlined in the scope of work), and In the Pipeline (potential projects and their current stage). The Project Template provides the basic structure for a new project-specific base. It should be copied when starting a project base from scratch. Each emLab project has its own base. Teams can customize the number and content of the spreadsheets in their baseas they see fit, but at a minimum, must include the spreadsheets and columns specified in the Project Template base (see ‘required information’ below). If you are working on a smaller project that does not require its own base, add it to the Special Projects base. 1.4.3 Customizing your Project Base It is up to you and your team to determine how to make Airtable work for you. Outlined below are different ways to customize your base. Feel free to look at other bases for inspiration! 1.4.3.1 Required Information To maintain transparency within the team and keep track of who is working on what to foster collaboration, all projects should keep their Airtable base up to date. The Project Template provides the basic structure for a project specific base. To copy, click on the arrow in the bottom right hand corner and select “duplicate base.” Then rename the base and customize it to the needs of your team. The following tabs are required in your project base: Deliverables: all deliverables stated in the scope of work Pro tip: add a number to the beginning of each deliverable so that when you group by deliverable in the activities tab, they show up in order instead of alphabetically Activities: steps to achieve the deliverables. These are a level above individual tasks. For example if the deliverable is a report, an activity could be a literature review, and tasks would be specific components of the literature review. Within the Activities tab, we employ a RACI chart to track responsibilities, which is broken into: Responsible/Assigned to: who is responsible for doing the actual work for the task Accountable: who is held accountable for the success of the task and is the decision maker Consulted/Reviewer: who needs to be consulted for additional input or review Informed: who needs to be kept in the loop on project progress Team: list of team members with their project roles. This will be linked to the activities tab so you can see who is responsible and accountable for different activities. 1.4.3.2 Additional Tab Ideas Tasks: more detailed steps of how to reach an activity; could be day-to-day tasks Note: you can link a tasks tab with the activities tab to see how they feed into one another Datasets: way to keep track of all the datasets going into an analysis, if they have been collected, and what their priority is Example Analyses: tracking different versions of a model run to keep track of progress and outputs of different simulations run Example Murder board: tracking gaps in analysis On the back burner: bonus analysis that can be added on if there is extra time Stakeholders: tracking groups of stakeholders, what you want their feedback on, and when in the project you want their input Conferences and events: helps track potential conferences or events to present your work at Papers: if a project is planning to write multiple papers, dedicate a specific tab to tracking the papers and their progress Questions for PIs/team: place to keep track of unanswered questions 1.4.3.3 Column Options There are 25 column type options within Airtable. The ones we most commonly use are: link to another record (links information from different tabs within the same base; you can’t link information across bases), single line text, long text, attachment, checkbox, multiple select, single select, and date. Pro tip: enable rich text formatting for the long text option to add checklists, bold and italic text, bullets, etc. Once enabled, double-click on the box you want to edit and select the ¶ symbol to see all of the formatting options. 1.4.3.4 Ways to View your Base Different views There are 5 different ways to view your base: grid, calendar, gallery, kanaban, and form, which can be explored here. Pro tip: click on “Row Height” in the menu row to change the height of rows and therefore wrap text. Filtering This works like the normal filtering function in spreadsheets. You can sort A to Z, by date, exclude records with certain names, etc. Grouping Grouping allows you to bucket your spreadsheet by field type. You can group by deliverable, complete v. not complete, research track, etc. Example: Expanding a record Within each tab, you can expand records in the first column to view all its information at once by clicking the two opposing arrows before the text. This option combines information from all of the columns into an easy to read card format. "],["1.5-google-shared-drive.html", "1.5 Google Shared Drive", " 1.5 Google Shared Drive We have all experienced the moment where we can’t remember where a Google Doc is. To help solve this problem, we created an emLab Shared Google Drive, which is a centralized space for all of our documents to live within a shared file structure. Unlike files in My Drive (your personal Google Drive account), files within the Shared Drive belong to the team instead of an individual. So even if people leave, the files stay exactly where they are and aren’t lost with that person’s account. The structure of Shared Drive is detailed in Section 2. You can read more about Shared Drives here. 1.5.1 Sharing Files Note: for Shared Drive, you either add someone to the entire Drive OR share individual files with them. You cannot share folders within a Shared Drive with external collaborators. Members of the Shared Drive can see all folders and files within the Drive. If you want to send a quick link to someone who is part of the Shared Drive, simply copy the URL from your browser. There is no need to create a shared link if you are sending it to someone with access to the emLab Shared Drive. If you want to share files with people outside of the emLab Shared Drive, that is still possible. Within a document, click on Share in the upper right hand corner. From there are two options: either add people’s emails or create a shared link. 1.5.2 Linking the Shared Drive and your Computer 1.5.2.1 Why Install Drive File Stream? As explained above, using a Shared Drive means that individuals don’t “own” the files. All files are instead owned by the emLab Shared Drive, which lives “on the cloud”. You can access these files through a web browser, just as you would access them on Google Drive. However, it is nice to have the files directly in your computer. To do so, you’ll need to install Drive File Stream, Google’s new system that will partially replace Google Backup and Sync There are important distinctions between Backup and Sync and Drive File Stream. As the names indicate it, Backup and Sync syncs and stores Drive content locally on your computer (that is, it takes up space in your hard drive). Drive File Stream, however streams all files and folders from the cloud. You can think of these as owning a DVD vs. streaming the movie from NetFlix. You can read more on the differences between these approaches here. 1.5.2.2 Step-by-step Installation Go to Get started with Drive File Stream and select the appropriate installer for your operating system. Note: Make sure you are on your ucsb account Download Drive File Stream If you are on a Windows machine, execute the googledrivefilestream.exe file. If you are on MacOS, run the googledrivefilestream.dmg file and then run the GoogleDriveFileStream.pkg file. Follow instruction on the helper (accept all defaults) Once the installation is complete, you’ll need to sign in to your account. To do so: Click on the Drive File Stream icon () On Windows machines, it should be on the bottom right corner of your screen On MacOS, it should be at the top right of your screen Sign in with your ucsb account After signing in, the application will launch a short 4-screen overview of the product The last screen should look like the screenshot below. Click on the button to open your GoogleDrive folder and access your files four-screen overview Once that finder / explorer opens, you should be able to see two folders: 1) My Drive and 2) Shared drives. The first folder will contain files owned by you (that is, your normal Google Drive Files stored on your ucsb account). The second folder will contain all shared drives you are part of. By navigating to that folder, you should see the emLab shared drive, containing all the folders mentioned earlier. A general structure of the GoogleDrive folder is shown below: Google Drive |__ My Drive | |__ Whatever files you have on your GoogleDrive |__ Shared drives |__ emlab |__central-emlab-resources |__communications |__data |__projects |__strategy 1.5.3 Other Considerations 1.5.3.1 Actual location of the Google Drive folder on my computer Remember that File Stream will stream, not save files to your computer. Therefore, there will be no folder directly under your devices / hard drives. Instead, your computer will detect the Google Drive folder as if it were an external device connected to your machine. On a Mac, the folder will therefore appear on your desktop, or under the Devices tab of your Finder. On a Windows machine, it will appear under This PC, next to your hard drive and any other connected devices. Location of the Google Drive folder depending on your operating system. Left panel shows location of the folder on Mac, and right shows the location of the folder on Windows. 1.5.3.2 I also want to link a personal gmail account If you also want to sync files from your personal gmail account, you’ll need to use Backup and Sync. You can install it, and make sure to sign in using your gmail account, not your ucsb account. Everything else should stay the same. "],["1.6-git-and-github.html", "1.6 Git and GitHub", " 1.6 Git and GitHub Since most of our projects at emLab involve code, we use Git to track changes made to our code and faciliate collaboration by merging changes made by others, and GitHub to organize, share, and backup our code. This section provides a brief overview of how Git and Github work, how to install them on your computer (and how to join the emLab GitHub page), and some general guidelines for how to use GitHub to organize code associated with emLab projects. 1.6.1 What are Git and GitHub? Git is an open-source version control system designed for programmers. Git can operate as a standalone program on your computer, but can also operated through many other programs (or “clients”). GitHub (really github.com) is a hosting service that provides online storage for your Git-projects. Think of Git as a little creature that keeps a record of all of the changes made to a file stored on your computer, and GitHub as a safe place on the internet that the little creature can go and put a copy of that file (and the changes you’ve made) when you tell it to do so. There are a number of good tutorials with more information on how Git and GitHub work (as well as how you can set them up to sync directly through other programs such as RStudio). The Ocean Health Index team at the National Center for Ecological Analysis and Synthesis (NCEAS) here in Santa Barbara created a very detailed data science training that includes two excellent tutorials on setting up and collaborating with GitHub: GitHub Collaborating with GitHub If you’re new to using Git and GitHub, the two tutorials listed above are a great place to start since NCEAS and emLab often operate in a similar way. Additionally, see the Software Carpentry’s lesson for the Git novice. If you primarily use (or will use) R for coding, Jenny Bryan also has an excellent tutorial specifically about how to integrate Git and GitHub with R: Happy Git and GitHub for the useR If you’re interested in learning more about all of the functionality GitHub has to offer, the Openscapes team at NCEAS has also tutorials on how to use GitHub for publishing code and for project management: GitHub for Publishing GitHub for Project Management 1.6.2 Helpful Terminology Git and GitHub use some weird terms that might be unfamiliar. Before installing and setting up Git and GitHub, here are a few key terms you may come across: repository (“repo”): a collection of files pertaining to the same project, document, goal, etc. Generally there’s a single repository for each project at emLab containing all of the code associated with that project. This repository can be organized with multiple folders and subfolders. commit: a set of changes made by a user to one or more files in a repository that the user wants to prepare to send to GitHub. push: the action of sending a commit from your local machine to the remote GitHub directory. pull: the action of retrieving any commits that have been made to the repository and are stored in the remote GitHub directory but are NOT on your local machine. 1.6.3 How to Install Git and GitHub Most of tutorials listed above include detailed instructions on how to install Git and GitHub. The short version (and steps specific to getting incorporated with the emLab GitHub page) are listed below. For more detailed instructions, please refer to the tutorials listed above. Create a free GitHub account Notes: use your @ucsb.edu email make sure you remember your username and password, you’ll need this later Landing page for GitHub Since GitHub is a company, and is used by many different types of organizations in many different industries, they offer a few different pricing schemes/deals. As an individual, once you create a username and sign up for an account, you get an unlimited number of free public and private repositories, but the number of external collaborators allowed in private repositories is limited to three. GitHub also offers a “Pro” plan for $7/month giving you unlimited external collaborators on all of your private repositories. However, for students, faculty, and research staff, or official nonprofit organizations and charities GitHub waives this fee through its GitHub Education and GitHub for Good programs. Good news! emLab qualifies as an educational organization through the GitHub Education program, and as a UCSB staff member you qualify for the individual educational discount. So, once you’ve signed up for a free account on GitHub… Go to the GitHub Education page and register as a researcher (Note: this is why you should use your @ucsb.edu email for step 1). Landing page for GitHub Education Click on the “Get benefits” link in the top right-hand corner and follow the directions to upgrade your account to a “Pro” account for free. You may need to take a picture of your UCSB ID card to submit as part of this process. GitHub may also periodically ask you to re-verify your eligability to qualify for this program. Send Erin O’Reilly a Slack message (or an email if you must… eoreilly@ucsb.edu) with your new GitHub username so you can be added to the emLab GitHub page! emLab GitHub page The emLab GitHub page is where the repositories for all emLab projects live (more on this later), and once you are a member of the organization you will be able to create new public and private repositories that appear here (as well as on your personal page). Install Git If you’re very very lucky, Git will already be installed on your computer. Open the shell for your opperating system. If you’re using Mac OS X, this is called Terminal. If you’re using Windows, you have multiple types of shells, but you should be using a Git Bash shell (NOT Power Shell). The easiest way to find out whether Git is already installed on your machine is to type the following: git --version ## git version 2.20.1 (Apple Git-117) If it returns a version number, you already have Git installed! However, if it returns something like git: command not found, you need to install Git. There are a number of different ways to install Git. Stand-alone installers exist for Mac OS X and Windows. If you’re using Linux, you probably already know how to install Git. If you’re using Mac OS X, Git can also be installed as part of the XCode Command Line tools, or you can also install it using Homebrew. If you’re interested in either of those options, follow the cooresponding directions in Jenny Bryan’s tutorial. If that sentence doesn’t mean anything to you, download the installer from the link above and follow the prompts. Once you’ve installed Git via whichever method you’ve chosen for your operating system, open the shell again and retype the same command to verify that the installation was successful: git --version ## git version 2.20.1 (Apple Git-117) It should now return a version number. Tell Git who you are Git needs to know a little bit more about you in order to play nicely. In particular there are two things that it’s helpful to configure: 1) The name that will be associated with any commits you make, and 2) the email address asssociated with your GitHub account. To set these two things, type the following into the shell using your name and email: git config --global user.name &#39;Jane Doe&#39; git config --global user.email &#39;jane@example.com&#39; The user name input here should be your full name (i.e. it does not need to be the same as your username for GitHub), but the email DOES need to be the same as that associated with your GitHub account. You can then check to make sure these were entered correctly by typing: git config --global --list Optional: Store your credentials (so you don’t have to type your password every time): Git will sometimes want to make sure you are you when performing certain operations. For example, when cloning a private repo or when you want to push changes to a repo. If you don’t want to do this every time, you can tell Git to remember your password too. You can read more about Git’s credential management here. On your terminal, navigate to a repository on your computer. (You can also use the Terminal pane within RStudio) and type the following into the shell: # Tell git to use the credential.helper git config --global credential.helper store # git pull (or git push) will prompt you to enter your password git pull What we just did was to tell Git to store our credentials. So, after typing them this one time, you should not need to type it again. Optional: Install a client for Git to make your life easier If you actually tried step 6 and you’re still reading this, you probably don’t usually spend a lot of time running commands in the shell and the last step didn’t make a lot of sense. If that’s the case, you might want to also install a Git client in order to help you visualize what Git is actually doing. You do not need a Git client to take advantage of version-control functionality of Git, as everything can be done using the shell (as in the previous step). However, the shell is not user-friendly. There are a number of Git/GitHub clients that you can download to interact with Git and GitHub in a more visual way. If you use RStudio, there is a very basic Git client built in that may be enough to get you started (more on this later). Other nice free Git clients include: GitKraken (available for all platforms, plus the logo octopus is pretty sweet…) GitFiend (cross-platform) SourceTree (has some problems on Mac OS X) GitHub Desktop (not available for Linux) GitUp (only for Mac OS X) There are many more. See Jenny Bryan’s tutorial if you’re not satisfied with those choices. Once you’ve installed a Git client, follow the directions to connect to your GitHub account. Once you’ve done this, try opening the local version of the repository you made in step 6, and notice the nice visual representation of the changes you made. 1.6.4 General Guidelines for using GitHub at emLab In general, each emLab project should have its own repository. There may be some cases in which multiple repositories may be associated with the same project, but this should be avoided if possible. The project repository should be created within the emLab GitHub page (exceptions may exist for example if a partner organization requires that the project repository be created within their organization’s GitHub page). Repositories can be made public or private when they are created (depending on the nature of the project) while the project is ongoing, but should be made public when the project is complete. Since many previous (and ongoing) projects were created within the personal GitHub pages of emLab members, the ownership of these repositories should be transferred to the emLab GitHub page at the conclusion of the project if possible. In order to ensure that the relevant researchers are notified of issues and other activities, researchers should “watch” or “subscribe” to repositories in which they work. "],["1.7-zotero.html", "1.7 Zotero", " 1.7 Zotero emLab uses Zotero to collaboratively collect, organize, and cite publications. We organize publications into Group Libraries which can be at the project or paper level. Our account – emlab-ucsb – has unlimited storage and there is no limit to the number of people you can add to a library. In Zotero, storage for a library only counts against the owner, so the emLab account must be the owner of the library for access to unlimited storage. 1.7.1 Making emLab the Owner of a Library If creating a new group library, email Erin (eoreilly@ucsb.edu) and she can directly create the library under the emLab account. If you are the owner of an existing library and would like to transfer ownership to emLab, first invite emlab-ucsb to be a member of the library. Once that invitation has been accepted, then go to “Group Settings” for the library and in the bottom right of the page, select transfer ownership to the emLab account. 1.7.2 Saving Publications to a Library To use Zotero properly, you need to install the Zotero Connector for your browser in addition to the Zotero desktop app (see this page for downloads). The most convenient way to save publications to Zotero is through Zotero Connector. The Zotero Connector allows you to add publications from your browser to Zotero with a single click. The “save” button shows up in the upper right corner of your browser and the symbol appears differently depending on the type of reference you are viewing (e.g. book, PDF, webpage). After you click the save button, a popup will appear that allows you to select the library to save the reference to. Read more about saving items to Zotero here. 1.7.3 Zotero Integration with GoogleDocs and Word Zotero is a great resource for adding references to a manuscript. It is integrated into both Word and GoogleDocs for easy use. As a note, you have to take extra steps to ensure that your references remain linked when switching between Word and GoogleDocs. See this document for more information about moving documents with Zotero citations between word processors. 1.7.3.1 Adding a Citation to your Document In both Word and GoogleDocs, Zotero shows up as a separate tab/menu item for you to choose from. For example, this is the Zotero tab in Word. To add an in-text citation, place your cursor after the statement you want to reference and then click on “Add/Edit Citation.” This brings up a Zotero search bar that allows you to search for a reference within your libraries based on a keyword or author. As you type, Zotero brings up options that you can select from. Once you select the citation, either insert a comma to add another citation or select enter to add the reference. Note: If you are adding a citation for the first time to a document, you will be asked to select the citation style. This allows you to add a reference style based on your target journal. 1.7.3.2 Adding a Bibliography to your Document The most amazing thing about Zotero is that it automatically generates a bibliography for you based on your in-text citations. To add a bibliography, simply place your cursor where you want to add your reference list and select “Add/Edit Bibliography.” It will add all of the references for you based on the citation style selected. "],["2-file-structure.html", "2 File Structure", " 2 File Structure This section details the emLab’s organizational stucture for files and data stored on Google Drive and code stored on GitHub. "],["2.1-folder-naming.html", "2.1 Folder Naming", " 2.1 Folder Naming There are some general style conventions that should be used when naming folders. For any new folder, be desciptive but concise, avoid spaces, avoid uppercase (or camel-case), and avoid special characters other than -. Words should be separated with -, so an example folder name could be blue-paradox-paper. This folder naming convention should be used for any folder added to the emLab Shared Drive or GitHub. "],["2.2-shared-drive-files.html", "2.2 Shared Drive Files", " 2.2 Shared Drive Files 2.2.1 General Structure The emLab Shared Drive is organized into five main folders: central-emlab-resources: includes meeting and event information, onboarding materials, information about travel reimbursements, and the team roster communications: includes the blog schedule, Adobe design projects, PowerPoint templates, photo repository, emLab logos, and publication and media tracking data: includes the emLab data directory and all datasets we work with (see section 2.2.3 for more on this) projects: includes information on past and current projects, and project management guidelines strategy: includes information about emLab’s strategic plan A full table of contents can be seen here. 2.2.2 Project Folder Structure Each project folder must contain the following 6 folders: data: This data folder will contain a data_overview spreadsheet and all of the intermediate datasets as well as output datasets associated with the project (see section 2.2.3 for more on this). Be sure to also add a copy of your final datasets to the emlab/data folder and data directory. deliverables: final reports, paper manuscripts, other final deliverables not related to data outputs grant-reporting: grant reports for funders meetings-and-events: meeting notes, agendas, documentation for workshop/event planning presentations: any presentations created for the project project-materials: everything else that does not fit into one of these folders (i.e. drafts of methods, literature review, etc.) From here, each project can add sub-folders as they see fit within these 6 folders. 2.2.3 Data Storage As stated above, there are two locations in which data can be stored. The two locations are Google Drive/Shared drives/emlab/projects/example-project/data and Google Drive/Shared drives/emlab/data. This may seem confusing and redundant, but this section explains the differences between these two locations. As a short summary: example-project/data may contain raw, cleaned, intermediate, and output files for a given project, and will be used as the “workspace” while the project develops. On the other hand, emlab/data contains only (raw) input and output data from a finalized project. More detail is provided in the subsequent sections. 2.2.3.1 example-project/data All data used in a project should live in this project-specific repository. To help keep track of project data, we highly recommend creating a data_overview spreadsheet (see section 3.3.2 for more information). This overview spreadsheet will provide a centralized summary of the data inputs and outputs for a project, and also allow teams to keep track of the status of adding the data to the emlab/data folder and creating the necessary metadata documentation. This repository will typically contain subfolders for raw, processed (or clean), and output, although each team might make slight modifications to this structure to suit their needs. To illustrate how each of these subfolders might be used, consider the following. A team may receive data from partners, extract data from external sources, compile survey responses, create a new dataset from a literature review, or use results from previous projects as input. These data are termed “raw data” and should never be directly modified - all of the errors, mistakes, and gremlins should be kept in the original versions. Instead, they should be processed / cleaned, and then exported as “clean data” that is actually used in analyses. The script used to do the processing / cleaning then acts as a reproducable record of everything that was done to the raw data. Suppose that a team working in Montserrat is tasked to perform a stock assessment on lobster populations and receives a database of lobster landings from the government. These data are stored as an excel spreadsheet, and will surely contain many mistakes that need to be fixed prior to running anly analyses. The team will clean the data (preferabily, using a reproducible script), and then export a new version of the data in which the mistakes have been fixed. The team will then perform the stock assessment, and produce results before reporting back. Therefore, the project-level data folder could be subdivided into raw, clean, and output folders. The first one will contain the excel file recieved from the government. The second folder will contain the cleaned data (perhaps exported as a csv), which can then be used as input for analyses within this project. The output folder will then contain the stock assessment results that might be relevant to other projects. As stated above, since the output folder could contain information relevant to other projects, this data should be made available to other emLab projects once the project is complete. To do this, any output data (and raw data if it is not already there) should be moved to the emlab/data folder, as described below. 2.2.3.2 emlab/data As a general rule, this folder contains all data used and produced by emLab projects. The idea is to make it easier for people to find data that has been used in previous projects, as well as to use previous results as inputs for new projects. To illustrate types of data that should be in the emlab/data folder, consider the following. The RAM Legacy stock assessment database is key to many projects, and was used as input in the Costello et al. 2016 “upsides” paper. The “upsides database” is an output from the Costello paper, which has then been used as input for other projects. Therefore, the emlab/data folder contains separate folders for both the RAM and upsides datasets. This large central data repository has the potential to become messy. Therefore, it is important to follow some key guidelines to store the data. All datasets in this folder should be contained within their own folders that include at minimum the data and metadata files. For example, a file structure for the two datasets mentioned above might be: In the above example, the folder containing the upsides database is relatively straightforward with the metadata file and a single csv file. However, the folder containing the RAM database is more complicated as this is a dataset that is re-released every so often as a new version. Specific guidelines for organizing different types of data within the emlab/data folder are discussed in detail in section 3. "],["2.3-github-structure.html", "2.3 GitHub Structure", " 2.3 GitHub Structure 2.3.1 Project Repository Structure The structure of each emlab repository on GitHub will likely vary depending on the needs of the project, but the following structure is suggested as a starting point. A documents (or docs) folder may be useful for storing code files that are used to generate text-based documents or presentations. Types of files that might live here include things like markdown files. A results folder may be useful for storing plots or other types of results generated by the project. Some discretion needs to be used here, as some results may actually be considered to be “processed” or “output” data. However, results in the form of figures or workspace image files might live here. A scripts folder may be useful for storing the code files that do everything from processing the raw data to running the analysis and generating outputs. A functions folder may be useful for storing the code files in which functions that are used by many scripts many be stored. Different types of projects may require more or fewer folders and these are only meant to act as suggestions. Regardless, the structure of the repository should be sufficiently organized such that it can be easily navigated and understood by others by the time the project is completed. 2.3.2 A repo inside a repo Sometimes, a project may have more than one paper or analysis sections. On some corner scenarios, we might want to have multiple “paper folders” within a “project folder”. This would imply that we will have a repo inside a repo. If that is something that makes sense for you, your project, and your team, then git submodules are your solution. If you want to read more on when / how to use submodules, visit the documentation page here. Including submodules in your workflow is simple. Here’s an example. you are working on a big project called “Blue Future”. The project has six PIs, 13 Research Specialists, two PostDocs, and three PhD Students. After a long kick-off meeting, the team realizes that the project will produce two papers and a ShinyApp. You are all determined to keep everything on the same folder, but correctly categorized and organized. As such, you go to GitHub and create the following four repositories: Blue Future Paper 1 Paper 2 ShinyApp You’ll clone the Blue Future repo into your comuter, using the usual: git clone https://github.com/emlab-ucsb/BlueFuture.git Now, instead of cloning the repos for each paper and the app into their own folder, you’ll navigate into your local BlueFuture folder. Then, instead of cloning them there, you can just do: git submodule add https://github.com/emlab-ucsb/Paper1.git This will clone the Paper 1 repo, but not without first telling the BlueFuture repo about it (just so that you don’t end up tracking things twice). You can repeat the operation for Paper2 and ShinyApp. That’s it! "],["3-data.html", "3 Data", " 3 Data Data should be managed and shared properly to make it useful in research, to promote transparency and facilitate reproducibility, and to ensure the credibility of research. This includes such practices as transforming data into a tidy format, storing data in open file formats, and providing data documentation. All emLab data, unless restricted, should be stored in the emLab Shared Drive (location dependent on type of data, see Section 2.2.3). Recommended readings: Borer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Ellis, Shannon E., and Jeffrey T. Leek. 2018. “How to Share Data for Collaboration.” The American Statistician 72 (1): 53–57. https://doi.org/10.1080/00031305.2017.1375987. Goodman, Alyssa, Alberto Pepe, Alexander W. Blocker, Christine L. Borgman, Kyle Cranmer, Merce Crosas, Rosanne Di Stefano, et al. 2014. “Ten Simple Rules for the Care and Feeding of Scientific Data.” PLOS Computational Biology 10 (4): e1003542. https://doi.org/10.1371/journal.pcbi.1003542. Hart, Edmund M., Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H. Woo, Naupaka B. Zimmerman, and Jeffrey W. Hollister. 2016. “Ten Simple Rules for Digital Data Storage.” PLOS Computational Biology 12 (10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097. Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (March): 160018. https://doi.org/10.1038/sdata.2016.18. Also see the resources available at DataONE. "],["3.1-data-file-naming.html", "3.1 Data File Naming", " 3.1 Data File Naming “Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread” - Hadley Wickham Coding styles guides provide teams with a shared language that promotes consistency, improves collaboration, and makes code easier to write as it simplifies the number of decisions we need to make on a daily basis (e.g., do I use - or _ to name this file? ). All style guides are opinionated and while they all intend to make code easier to read and write, some decisions are arbitrary. Thus, instead of us at emLab creating our own, we want to adopt the “proven and tested” Tidyverse style guide. This styles guide has good advice not only for naming files but also for naming functions, objects, and general best coding practices. We encourage all emLab members to read the guide and work towards adopting it. When it comes to file naming, best practices include: Names should be descriptive and meaningful. Many coding interfaces now have autocompletion tools, so the length of the filename is less of a concern. Avoid spaces and upper case letters. Some operating systems are case sensitive so in the interest of collaboration, let’s use only only lower case. Use _ to separate words in filenames. Do not use -, ., or any other special characters. Strive to use verbs for function names. If files should be run in a particular order, prefix them with numbers. If you’ll have more than 10 files left pad with zero (e.g., 01_get_data.R) "],["3.2-metadata.html", "3.2 Metadata", " 3.2 Metadata Metadata is data about your data. It includes information about your data’s content, structure, authors, and permissions to make your data interpretable and usable by your future self and others. EVERY data file should be accompanied by a metadata file. This includes files that people tend to overlook or think are not useful for the broader team. For example, if you’re using Google Sheets to keep track of literature or data reviews for a specific project, these documents should also have some form of accompanying metadata. 3.2.1 Metadata Standards We use “readme” style metadata, named _readme_datafilename, and stored in the same folder as the data file. Create one readme file for each data file. Download and use this template to create your readme file (when one is not already available). Name the readme _readme_datafilename and save as a text file. Format the readme so it is easy to understand (use bullets, break up information, etc.) Use a standardized date format (YYYY-MM-DD) We acknowledge that it may not always be feasible to draft robust metadata immediately when a dataset is first uploaded to the emlab/data folder. In this case, a minimal readme file can be created as a temporary placeholder that contains the following information: your name, contact info, a very brief (1-2 lines) description of the data, and a note on how you obtained them. Please refer to the Data Directory subheading for further instruction on how to incorporate this form of temporary documentation into the emLab Data Directory. For files that may be considered more “internal notes” than datasets (Google Sheets example mentioned above), please ensure that some sort of metadata is present. One alternative to a “readme” file is to create an extra tab on the Google Sheet labeled “metadata”. Here, you can include information on the column names (column 1) and their definitions (column 2). This allows collaborators and team members to easily interpret the columns and use the dataset appropriately. 3.2.2 Where to Store Metadata All readme metadata files are stored in the folder that contains the data file in the emLab Data Directory. "],["3.3-data-directory.html", "3.3 Data Directory", " 3.3 Data Directory 3.3.1 emLab Data Directory All emLab data is stored in subfolders of the Data folder on the emLab Team Drive (emlab/data). To document these data, we use the emLab Data Directory that includes key, standardized information from each readme metadata file. Every data file in the emlab/data folder has a record (row) in the emLab Data Directory. The emLab Data Directory file contains two sheets: (1) Data directory (the record and standardized documentation for each data file); (2) Metadata (information needed to populate the Data Directory, i.e. the meta-metadata) In the case of placeholder metadata (as described in the Metadata section), only the following columns should be filled out: folder, filename, contact, and summary. This (mostly blank) row serves two purposes: 1) it retains some of the searchability function for that dataset and 2) it serves as a visual reminder that those datasets are in need of more robust metadata development. Column Description Domain Climate/Energy; Land; Ocean; General; Other [drop down menu] Description A few word description (e.g. SST US 2017); max 5 words Folder Name of folder containing data Filename Name of data Year Year of publication Version Sub category of year; NA if not applicable Project Project name that used these data (can have multiple listings) or ‘General’ if widely used (e.g. FAO data), hyperlinked to Google Drive/Box folder Code Link to Github repo or wherever code is stored Data Stage raw’ if raw data; ‘final input’ for the input data used for the analysis; ‘output’ for what was used for the project and/or published [drop down menu] Filetype File extension (e.g. csv; tif; rds); note: do note include ‘.’ Citation Hyperlinked reference to publication or online resource or contact for individual/group data author URL Link to original data source Extent global; regional; national; local [drop down menu] Resolution Resolution of spatial data (in degrees) Permissions open = open source/open access; restricted = need author permission; secure = confidential data and likely involves a DUA or NDA [drop down menu] Start year Data set start year; numeric End year Data set end year; numeric Source e.g. emLab; FAO; Rare Contact Name and email of contact person in emLab who used/stored data emLab reference Hyperlinked reference to emLab publication using data (can be NA) Keywords e.g. fisheries; fire; utilities; property value; VDS; MPA; oceanography; temperature; habitat; biodiversity (up to 5 per entry, separated by semi-colons) Summary Brief description of the data (1-2 sentences). Include years for timeseries; location/spatial extent for spatial data; key variables; resolution; sampling frequency; species; etc. Notes Other relevant information about data. Initial your entry (e.g. if it was processed (e.g. subset from a larger dataset); what specifically was done; are there suspicious data points?; note if there are issues; etc.) Any time you add a new dataset to the shared emLab data folder and directory, please message the #data-streamlining Slack channel so that others on the team know about the new dataset. 3.3.2 Project-level Data Directory We highly recommend that research teams create a data_overview spreadsheet for keeping track of project-related data (i.e. a separate spreadsheet stored in the project’s Google Shared Drive data folder). This centralized document can be used to document project-relevant information and communicate to team members datasets that have already been saved. This document can then be used to guide and simplify data migration to the emLab Data Directory once the project is complete. Suggested attributes include: File name Folder name Source of data Link where data was downloaded Description of data Name of the researcher who downloaded the data Data directory entry (complete, in progress, not started, etc.) Metadata sheet (complete, in progress, not started, etc.) "],["3.4-tidy-data.html", "3.4 Tidy Data", " 3.4 Tidy Data As researchers, we work with data from many different sources. Often these data are messy. One of the first steps of any analysis is to clean any available raw data so that you can make simple visualizations of the data, calculate simple summary statistics, look for missing or incorrect data, and eventually proceed with more involved analyses or modeling. As part of the data cleaning process, we recommend getting all data into a “tidy” format (also known as “long” format, as opposed to “wide” format). According to the Tidy Data Guide by Garrett Grolemund and Hadley Wickham, tidy data is defined as: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. These three features of tidy data can be seen in the following figure, also from the Tidy Data Guide: Once data are in this format, it makes subsequent visualization and analysis much easier. If you’ve ever worked with a file that has a separate column for each year (an example of “wide” format data), you know how hard that type of data format is to work with! As always, we recommend keeping a backup copy of the raw data you obtained from the original source, and using a reproducible script for transforming these data into a tidy data format. 3.4.1 Recommended Resources We highly recommend the chapter on Tidy Data from the book R for Data Science by Garrett Grolemund and Hadley Wickham. This guide is geared towards R users and provides helpful tips for transforming and working with data in R, but the concepts should be broadly applicable to other languages as well. For tips specific to Python, we recommend a blogpost by Jean-Nicholas Hould titled Tidy Data in Python. "],["3.5-data-formats.html", "3.5 Data Formats", " 3.5 Data Formats Data should preferably be stored and/or archived in open formats. Open formats maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them. The exact format will vary depending on the file type, but some common examples are comma separated value (.csv) as opposed to closed format Excel spreadsheets (.xls) for tabular data, and .pdf or .odt (Open document format for office applications) as opposed to Word documents for final versions of reports. When saving final geospatial products, we recommend geopackage (.gpkg) which has several advantages over the commonly used shapefile format (more here): Single file Open format (shapefile is ESRI-dependent) Fewer limitations: e.g. no limits on attribute file names (shapefile has max. 254 characters), max file size 140 TB! (shapefile max. 4GB). Geopackage files can be written and read in R, QGIS, and ArcGIS. "],["3.6-data-use-agreements-and-confidential-data.html", "3.6 Data Use Agreements and Confidential Data", " 3.6 Data Use Agreements and Confidential Data 3.6.1 The process for establishing a Data Use Agreement or Non-Disclosure Agreement At project launch, the project manager and the rest of the project team should determine if a Data Use Agreement (DUA; common) or Non-Disclosure Agreement (NDA; not common) is necessary. Any project that will involve the use and sharing of data that is not publicly available should establish a DUA or NDA. Start this process right away. DUA is preferred if possible; only do NDA if necessary or requested by partner. These agreements should go through UCSB’s Office of Technology &amp; Industry Alliances (TIA). This page provides guidelines for establishing a DUA. The first step in establishing a DUA is to fill out a DUA Request Form and send it to Jenna Nakano (nakano@tia.ucsb.edu). This page provides guidelines for establishing an NDA. The first step in establishing an NDA is to fill out a NDA Request Form and send it to Jenna Nakano (nakano@tia.ucsb.edu). CC emLab’s Amanda Kelley (aekelley@ucsb.edu) on all emails relating to the DUA or NDA. Once a request form has been sent, TIA will help produce standardized agreements that are based on answers to these forms. Alternatively, TIA can also review DUAs or NDAs that partners share. 3.6.2 Data storage options emLab is generally happy to work with partners to determine the most appropriate method for data storage. Some partners may have specific data storage requirements that will be laid out in the DUA or NDA. If the DUA or NDA does not have specific data storage requirements, we recommend one of the following three approaches depending on how sensitive the data are: For data that are not confidential or sensitive, data should be stored in a project-specific directory on the emLab Team Drive. Only emLab PIs and full-time emLab staff will have default access to this data directory. Any additional access for postdocs, students, or other external collaborators will only be granted on an as-needed basis and only after the collaborator has read the DUA or NDA. This option is used for the vast majority of emLab projects. For confidential or sensitive data, the primary recommended approach is the UCSB Knot Cluster through the Center for Scientific Computing. We recommend using this approach if your DUA or NDA allows for it. To set this up, use the request form here. Nathan (Fuzzy) Rogers (Research Computing Administrator, fuz@mrl.ucsb.edu) and Paul Weakliem (CNSI Research Computing Support, weakliem@cnsi.ucsb.edu) are good resources for questions. Anyone storing sensitive data with the knot cluster should ensure that UCSB locks the data so that they remain private. For confidential or sensitive data, the second option is storing data on the Secure Compute Research Environment (SCRE) at UCSB’s North Hall Data Center. The SCRE “is a private, secure, virtual environment for researchers to remotely analyze sensitive data, create research results, and output results and analyses.” We only recommend this approach if your DUA or NDA requires data be stored in a secure facility like the North Hall Data Center Setting up an SCRE gives you access to a secure virtual desktop that comes pre-loaded with applications such as R and R Studio. You can make a request for an SCRE using the request form here. UCSB IT will help set this up. You can follow up with questions at scre-support@ets.ucsb.edu Requests are usually fulfilled within one week. Further information can be found in the SCRE user guide. Jennifer Mehl (Information Security Analyst, jennifer.mehl@ucsb.edu) is another good resource for questions. The SCRE has a number of important limitations: it is relatively slow; it is very difficult to access for non-UCSB collaborators; it can only be set up after the NDA/DUA is established; it can be difficult to install Stata and may require an individual license; any non-standard R packages need to be installed manually by a UCSB person managing the SCRE 3.6.3 Other best practices Regardless of the data storage option chosen, we recommend several additional best practices: High-level metadata for all datasets should be added to the _emlab_data_directory. See this section in the emLab SOP for further description of the emLab data directory. This will help emLab be internally transparent in how we are using data for different projects, even if the entire group doesn’t have access to the data. For datasets that are confidential, ensure that the “Permissions” column is set to “Secure = confidential data and likely involves a DUA or NDA.” Researchers should consider anonymizing individual-level data before publicly releasing the data (see this R package as one example for how to do this). "],["4-code.html", "4 Code", " 4 Code All data processing and analysis should be performed with code (i.e., avoid spreadsheets), and all code should be packaged in scripts that are version controlled and follow a style guide. Using code and scripts allows for better organization, documentation, and reproducibility of analysis workflows. All emLab code should be stored in the emLab GitHub account. Recommended readings: Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk About Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928. Stodden, Victoria, and Sheila Miguez. 2014. “Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research.” Journal of Open Research Software 2 (1): e21. https://doi.org/10.5334/jors.ay. Wilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745. https://doi.org/10.1371/journal.pbio.1001745. Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510. "],["4.1-scripts-and-version-control.html", "4.1 Scripts and Version Control", " 4.1 Scripts and Version Control Code should be crafted according to the following guidelines: Use scripts Document scripts, but not too much Organize scripts consistently (see format below) Use Git to version control scripts Make atomic Git commits (see description below) Script files should be documented and organized in such a way to enhance readability and comprehension. For example, use a standardized header for general documentation and sections to make it easier to understand and find specific code of interest. Code should also be self-documenting as much as possible. Additionally, use relative filepaths for importing and exporting objects. Scripts should also be modular by focusing on one general task. For example, use one script for cleaning data, another script for visualizing data, etc. A makefile can then be used to document the analysis workflow. There is an art to this organization, so just keep in mind the general principle of making code easy to understand, for your future self and for others. Here is an example template for R scripts: # ============================================================================= # Name: script.R # Description: Visualizes data # # Inputs: data.csv # Outputs: graph.png # # Notes: - Use a diverging color palette for best results # - Output format can be changed as needed # ============================================================================= # Set up environment ---------------------------------------------------------- library(tidyverse) # Set path for Google Drive filestream based on OS type # Note - this will work for Windows and Mac machines. # If you use Linux, you will need to set your own path to where Google Drive filestream lives. team_path &lt;- ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Windows&quot;,&quot;G:/&quot;,&quot;/Volumes/GoogleDrive/&quot;) # Next, set the path for data directory based on whether project is current or archived. # Note that if you use a different Shared Drive file structure than the one recommended in the &quot;File Structure&quot; section, you will need to manually define your data path. # You should always double-check the automatically generated paths in order to ensure they point to the correct directory. # First, set the name of your project project_name &lt;- &quot;my-project&quot; # This will automatically determine if the project exists in the &quot;current-projects&quot; or &quot;archived-projects&quot; Shared Drive folder, and set the appropriate data path accordingly. data_path &lt;- ifelse(dir.exists(paste0(team_path,&quot;Shared drives/emlab/projects/current-projects/&quot;,project_name)), paste0(team_path,&quot;Shared drives/emlab/projects/current-projects/&quot;,project_name,&quot;/data/&quot;), (paste0(team_path,&quot;Shared drives/emlab/projects/archived-projects/&quot;,project_name,&quot;/data/&quot;))) # Import data ----------------------------------------------------------------- # Load data from Shared Drive using appropriate data path my_raw_data &lt;- read_csv(paste0(data_path,&quot;raw/my_raw_data.csv&quot;)) # Process data ---------------------------------------------------------------- # Analyze data ---------------------------------------------------------------- # Visualize results ----------------------------------------------------------- # Save results ---------------------------------------------------------------- Git tracks changes in code line-by-line with the use of commits. Commits should be atomic by documenting single, specific changes in code as opposed to multiple, unrelated changes. Atomic commits can be small or large depending on the change being made, and they enable easier code review and reversion. Git commit messages should be informative and follow a certain style, such as the guide found here. There is also an art to the version control process, so just keep in mind the general principle of making atomic commits. More advanced workflows for using Git and GitHub, such as using pull requests or branches, will vary from project to project. It is important that the members of each project agree to and follow a specific workflow to ensure that collaboration is effective and efficient. "],["4.2-style-guide.html", "4.2 Style Guide", " 4.2 Style Guide At emLab, we recommend using a consistent code style for each of the different programming languages we use. Recommended coding styles for a particular language are collated in what is known as a “style guide”. Style guides typically include standardized ways of naming script files, defining functions and variables, commenting code, etc. While emLab does not mandate the use of style guides, having a consistent code style allows all emLab staff to easily understand each other’s code, collaborate on projects, and jump in on new projects. We consider this an important aspect of collaboration, reproducibility, and transparency. Rather than re-invent the wheel, we leverage existing code style guidelines for each of the languages we use. Below is a summary of the code style guidelines we recommend for various languages: R - Tidyverse Style Guide, by Hadley Wickham Python - Google’s Python Style Guide Stata - Suggestions on Stata programming style, by Nicolas J. Cox Other languages - Google style guides for other languages "],["4.3-reproducibility.html", "4.3 Reproducibility", " 4.3 Reproducibility Prioritizing reproducibility when writing code code not only fosters collaboration with others during a project, but also makes it easier for users in the future (including yourself!) to make changes and rerun analyses as new data become available. Some useful tools and practices include: Commenting code: Adding brief but detailed comments to your scripts that document what your code does and how it works will help others understand and use your scripts. At the top of your scripts, describe the purpose of the code as well as its necessary inputs, required packages, and outputs. File paths: Avoid writing file paths that only work on your computer. Where possible, use relative file paths instead of absolute files paths so your code can be run by different users or operating systems. For R users, using R Projects and/or the here package are great ways to help implement this practice. Functions: If you find yourself copying and pasting similar blocks of code over and over to repeat tasks, turn it into a function! R Markdown: R users can take advantage of R Markdown for a coding format that seamlessly integrates sections of text alongside code chunks. R Markdown also enables you to transform your code and text into report-friendly formats such as PDFs or Word documents. Git and GitHub: As described above, Git tracks changes to files line-by-line in commits that are attributable to each team member working on the project. GitHub then compiles the history of any Git-tracked file online, and synchronizes the work of all collaborators in a central repository. Using Git and GitHub allow multiple people to code collaboratively, examine changes as they occur, and restore prior versions of files if necessary. "],["4.4-coding-pipelines.html", "4.4 Coding pipelines", " 4.4 Coding pipelines Often, a project contains multiple script files that are each run separately, but which produce or use interconnected inputs and outputs through a coding pipeline. They must therefore be run in the correct order, and any time a change is made to an upstream script or file, all downstream scripts may need to be run for their outputs to stay up-to-date. This presents a challenge to reproducibility, and also a challenge to anyone trying to work with or review complicated coding pipelines. Consider the following example, from Juan Carlos Villaseñor-Derbez’s excellent tutorial: Make: End-to-end reproducibility. Consider you have the following project file structure in a GitHub repository, where clean_data.R is run first and processes the raw data file raw_data.csv. When this is run, clean_data.R produces the cleaned data file clean_data.csv. plot_data.R is run next, which uses clean_data.csv and produces the final output figure1.png. data |_raw_data.csv |_clean_data.csv scripts |_clean_data.R |_plot_data.R results |_figure1.png |_raw_data.csv |_clean_data.csv These scripts must be run in the correct order, and any time changes are made to upstream data or scripts, downstream scripts must be run to update downstream data and results. To ensure reproducibility of your code, we recommend everyone employ one of the three following options when using a coding pipeline, with the three options in ascending order of recommendation from “minimum required” to “best practice.” Option 1 (minimum required): Provide clear documentation in the repo’s README.md file. This documentation should provide the complete file structure, including both scripts and data files, and should provide a narrative description of the order in which the scripts should be run, as well as which files each script uses and produces. Option 2 (better practice): Create a run_all.R script that runs all scripts in the correct order, and can be run to fully reproduce the entire project repo’s outputs. This run_all.R script should be described in README.md. Using the example above, this script could contain the following: # Run all scripts # This script runs all the code in my project, from scratch source(&quot;scripts/clean_data.R&quot;) # To clean the data source(&quot;scripts/plot_data.R&quot;) # To plot the data Option 3 (best practice): Use the make package, which uses a Makefile to fully describe the coding pipeline through a standardized syntax of targets (output files that need to be created), prerequisites (file dependencies), and commands (how to go from prerequisites to targets). The advantage of this approach is that it automatically runs scripts in the correct order, keeps track of when changes are made to scripts or data files, and it only runs the necessary scripts to ensure that all outputs are up-to-date. If using make, you should describe how to use it in the repo’s README.md. The Makefile uses the following syntax: target: prerequisites command Therefore, in our example, the Makefile would simply be the following, and the single command make would execute this: results/figure1.png: scripts/plot_data.R data/clean_data.csv Rscript scripts/plot_data.R data/clean_data.csv: scripts/clean_data.R data/raw_data.csv Rscript scripts/clean_data.R Please refer to Juan Carlos Villaseñor-Derbez’s very helpful tutorial for more information: Make: End-to-end reproducibility. "],["4.5-internal-code-review.html", "4.5 Internal code review", " 4.5 Internal code review Peer-review of each other’s code is a great method for mutual learning through exposure to different coding styles and packages, ensuring reproducibility, catching any bugs, and uncovering opportunities for doing things better. We recognize that this takes time, but we believe that the time commitment is worth it for us to all become better coders and write better code. As a best practice, we recommend that for any project that has at least two researchers, there should be a systematic review of any major coding elements by the researcher who did not initially write the code. By following the best practices outlined in the Coding Pipeline section of this SOP, the original coder can ensure that the reviewing coder knows exactly which pieces of code are critical for their review. Code reviews should be respectful and collegial in manner. The goal is to make all of our work better and make all us better coders, not to criticize or single anyone out. The original coder and reviewing coder should coordinate on the best method for feedback, whether that is through direct pull requests to the code, an email outlining suggestions, or a Google doc outlining suggestions. For further guidance on effective code reviewing techniques and guiding principles, we recommend using the Tidyteam code review principles. These were developed by the tidyteam for maintaining packages such as those found in the tidyverse and tidymodels. These guidelines are based on Google’s Code Review Developer Guide, another helpful resource. "],["4.6-exploratory-data-analysis.html", "4.6 Exploratory Data Analysis", " 4.6 Exploratory Data Analysis Much of the work emLab does is Exploratory Data Analysis (EDA), especially during the beginning stages of a project when we are familiarizing ourselves we new datasets and rapidly prototyping data wrangling and modeling approaches. The following quote from Hadley Wickham’s R for Data Science describes EDA as: EDA is an iterative cycle. You: 1) Generate questions about your data; 2) Search for answers by visualising, transforming, and modelling your data; 3 Use what you learn to refine your questions and/or generate new questions. EDA is a creative and exploratory process without defined rules. Hadley Wickham’s chapter on EDA is an excellent place to start for ideas, but we encourage creativity and exploration during this phase of a project. However, we recommend as a best practice that all EDA should include an “Interpretations, questions, and new ideas” section. The researcher doing EDA is poised in an excellent position for providing initial interpretations of the data, raising outstanding questions about the data, and generating novel insights and potential research questions. This section should contain the following information: Interpretations about what you found. Does it match our intuition? Is there anything surprising? Questions about the data. Are there any questions about how to use or interpret the data? Are there any potential problems with the data or analysis? New insights and research questions. Have you uncovered any new insights about the data or research? Have you discovered any new research questions that might be worth pursuing? By including a section in each EDA analysis that discusses these aspects, we can build insight generation into our workflows. This information should be in an easy to find location on the Team Drive, such as in a markdown-reports directory that contains EDA PDFs generated by R Markdown. This way, other members of the project team can read it, digest it, and respond with further ideas. "],["5-high-performance-computing.html", "5 High Performance Computing", " 5 High Performance Computing Certain analysis use cases require high performance computing resources: big data parallel computing lengthy computation times restricted-use data For analyses involving big data or models that take a long time to estimate, a single laptop or desktop computer is often not powerful enough or becomes inconvenient to use. Additionally, for analyses involving restricted-use data, such as datasets containing personally identifiable information, data use agreements typically stipulate that the data should be stored and analyzed in a secure manner. In these cases, you should use the high performance computing resources available to emLab, including cloud computing through Google Cloud Platform and the UCSB server clusters. Cloud computing incurs costs but is flexible whereas the UCSB server clusters are free but have some limitations, such as job queues. When to use Google Cloud Platform: need maximum computational flexibility When to use UCSB server clusters: costs are a concern using Stata using restricted-use data (depends on data use agreement) "],["5.1-google-cloud-platform.html", "5.1 Google Cloud Platform", " 5.1 Google Cloud Platform Google Cloud Platform (GCP) is a suite of cloud-based products that work together to provide robust and seamless solutions to high performance computing, big data storage, data analytics, machine learning and more. The platform is built on Google’s internal infrastructure and it’s known for it’s reliability, flexibility, speed, and a relatively low cost “pay-as-you” model. At emLab we mainly use three of GCP’s products: Cloud Storage, BigQuery, and Compute Engine and each year we have a limited amount of credits to cover the costs of using these tools for projects that require the storage and use of very large datasets, projects that require large computational power, or those that use Global Fishing Watch data. When it comes to high performance computing, Compute Engine is a very useful tool. It allows us to easily create custom-made virtual machines with the storage, memory, and number of cores needed for a given task. Virtual machines can run public images of Linux, Windows Server, and can also be used to deploy Docker containers. Starting, stopping, and deleting virtual machines is easy and fast which means we have full control on the amount of resources we use and get billed for. To get up and running with a virtual machine, Grant McDermott (SFG alumn and fellow) wrote this really good step by step tutorial. Here you will learn how to create, start, connect to, and stop a virtual machine in Compute Engine and how to install Rstudio server and Git. Importantly, you will also find a link that walks you through the installation of Google Cloud SDK command line utility (gcloud) which is a prerequisite to be able to speak to your virtual machine from your local terminal. When you install gcloud and autenthicate your credentials you will be able to set emlab-gcp as your project which will link you to emlab’s billing account. If you have not joined emlab-gcp please get in touch with and we will set you up! General guidelines for creating and running virtual machines: Give your VM a descriptive name associated with the specific project you will be using it for. Give your VM a static IP address. That way you can add it to your bookmarks and access it easily. Always turn off your VM when not it use. Remember we get charged for every minute it is on. Delete the VM once the project is finished. That way we keep things tidy. 5.1.1 Connecting to emLab’s Shared Drive A key consideration when using virtual machines is being able to access data stored elsewhere instead of having to copy data to the VMs hardrive. At emLab we use Google Drive as the central repository for datasets and project files, and fortunately, there are tools to connect to it from VMs created in Compute Engine. For VMs using GUI interfaces (e.g., Windows or MacOS), one can simply use filestream as one would locally. However, for headless VMs such as those running Ubuntu which - we create often to run Rstudio server - we need to use a FUSE filesystem over Google Drive called google-drive-ocamlfuse. At of November 2019, this workflow works for zesty and xenial distributions of Ubuntu. In a VM running Ubuntu, follow the installation instructions via PPA repository found here. sudo add-apt-repository ppa:alessandro-strada/google-drive-ocamlfuse-beta sudo apt-get update sudo apt-get install google-drive-ocamlfuse After installation you need to authorize google-drive-ocamlfuse and create a label for the connection. Labels are useful if you want to mount your personal drive as well as emLab’s Shared Drive. To authorize and create a label for our shared drive run: google-drive-ocamlfuse -headless -label emlab_drive -id ##yourClientID##.apps.googleusercontent.com -secret ###yoursecret##### Copy and paste the clientID and secret which can be found here under the file-stream OAuth 2.0 client ID. You will get an output like this: Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=##yourClientID##.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&amp;response_type=code&amp;access_type=offline&amp;approval_prompt=force Follow the prompt, grant access, and copy and paste the verification code in the terminal prompt. You should see a message saying the token was succesfully authorized. The last step to mount the team drive is to save the Shared Drive ID to the corresponding config file. This step is not necessary if you want to connect to your personal drive only. Open the config file in ~/.gdfuse/emlab_drive/config and look for the team_drive_id setting. Add our Team Drive ID (0AHyeeMXswgGLUk9PVA) and save the file. Now you are ready to mount the drive to a local folder! google-drive-ocamlfuse -label emlab_drive mountPoint "],["5.2-ucsb-server-clusters.html", "5.2 UCSB Server Clusters", " 5.2 UCSB Server Clusters The Center for Scientific Computing at UCSB provides resources and support for research involving high performance computing, including multiple server clusters for storing and analyzing data. Find out more information at http://csc.cnsi.ucsb.edu/. The clusters should be used if cloud computing costs are a concern. The cluster computing resources are free to emLab researchers, but they are a shared resource among UCSB researchers and involve job queues that can potentially delay analyses. However, most of the analyses typical of emLab projects require a small amount of resources relative to other users of the clusters. Additionally, the clusters should typically be used if using restricted-use data in research, though this depends on the terms of the data use agreement. Be sure to restrict access to any sensitive data on the clusters by changing the folder and file permissions appropriately. Some data providers may prefer to use cloud computing services for sensitive data because it may give them greater control over the uses of their data. If needed, these issues about where data can be stored and analyzed should be negotiated and resolved in data use agreements. A user guide for the clusters can be found at https://emlab-ucsb.github.io/cluster-guide/. "],["6-reports-and-publications.html", "6 Reports and Publications", " 6 Reports and Publications This section describes best practices related to emLab reports and publications. "],["6.1-emlab-affiliation.html", "6.1 emLab Affiliation", " 6.1 emLab Affiliation Since emLab is joint between MSI and Bren and also its own research entity, we recommend emLab staff use the following affiliations for any emLab publication: Marine Science Institute, University of California, Santa Barbara, Santa Barbara, CA, USA Bren School of Environmental Science &amp; Management, University of California, Santa Barbara, Santa Barbara, CA, USA Environmental Markets Lab, University of California, Santa Barbara, Santa Barbara, CA, USA Note that journals may list affiliation addresses slightly different than what is listed above. "],["6.2-reports.html", "6.2 Reports", " 6.2 Reports Many final deliverables for our projects include creating reports. To aid in the final report development, please see the Report template for information on a draft cover page, inside page, logos to use, and suggested citation. To ensure future searchability, please upload all final reports to Zenodo - a public repository that creates a DOI and link for all uploaded information - if the report can be made publicly available. If you don’t have an account, you can sign up here with your email, GitHub account, or ORCID. We have created an emLab repository for all of our reports to live. To upload to the emLab community repository, use this link and fill out all of the required information. "],["6.3-author-contribution.html", "6.3 Author Contribution", " 6.3 Author Contribution Transparency in authors’ contribution is a critical component of open science. Determining authorship is ultimately the responsibility of the project leads (i.e. principal investigator and/or first author), but because author contribution is not always straightforward, we follow McNutt et al. (2018) and others in recommending the Contributor Roles Taxonomy (CRediT) system. For a simplified version of this most relevant to our project infrastructure, feel free to use and modify our Author Contribution Template, which allows individuals to identify their contributions or to opt-out of participation in a paper. We recommend discussing authorship expectations early in a project to avoid future complications. "],["6.4-making-your-data-publicly-available.html", "6.4 Making Your Data Publicly Available", " 6.4 Making Your Data Publicly Available Generally, emLab researchers should be prepared to share a public GitHub repository (see Section 6.4) and a Dryad data repository with publications. The repositories contain code scripts and data (respectively) needed to conduct the study. Increasingly, journals are requiring both sets of documentation at the point of submission or publication. Dryad is an open-access repository of research data that makes data searchable, freely accessible, and citable. Data repositories do not need to be associated with a publication. Dryad is free for UCSB affiliates with a UCSB NetID. The following steps document the process for creating a Dryad repository. Review Dryad’s Best Practices website for more useful information. If you do not already have one, visit the ORCID site to obtain an ORCID identifier. You will need this to deposit data on Dryad. Use your ORCID username and password to sign into Dryad. Under My Datasets, select “+ Start New Dataset.” Fill in the required fields and describe your dataset. If your data is related to a manuscript in progress or a published article, you will be asked to provide the Manuscript Number or DOI. Upload your files. Note that while you cannot load a folder to Dryad, you can load a .zip file that includes a folder structure. This approach is suggested as it allows for improved organization. Review and submit. In this phase you can choose to “Enable Private for Peer Review,” which keeps your dataset private during your article’s review period. You will have access to a private dataset download URL that you can share with collaborators or the journal. If you make this selection, your dataset will not enter curation or be published. If you do not select “Enable Private for Peer Review,” you will submit your dataset to Dryad for curation. Skip to step 9 if you did not “Enable Private for Peer Review.” If you chose “Enable Private for Peer Review,” copy the URL to share with collaborators or the journal. Those who use this link may be asked to provide an email address in order to obtain the dataset. Note that in this case, users will receive an email from Dryad with instructions for how to download the dataset – notify users to check their Spam folder for this email. Once the manuscript is accepted, you can go back to your dataset and submit it to Dryad for curation. Make sure to incorporate any relevant changes that occurred during the revision period. During curation, Dryad will check your submission to ensure the validity of files and metadata. Dryad may contact you questions, suggestions, and/or identified problems. Once the dataset is approved, the Dryad DOI is officially registered and made public. Note that you can contact Dryad in order to delay the publication of your data until your publication date. Include DOI in your manuscript’s Data Availability statement, or consider citing it the References section. "],["6.5-preparing-a-public-github-repository.html", "6.5 Preparing a Public GitHub Repository", " 6.5 Preparing a Public GitHub Repository As with data, we strive to make all our code available. This provides a roadmap of converting input data into tangible results, which may be of interest for external people seeking to replicate our study or for internal emLabers seeking to understand what someone did a few years ago. 6.5.1 Documentation One of the most important things to include in the repo is a README.md file. This will be automatically displayed as rendered markdown on GitHub, and should provide a simple explanation of what’s in the repo, how to run it, and how it was run in the past. If possible / necessary, you might want to include a file structure (Take a look at using startR::create_readme() for automating this). If relevant, you might want to include the title of the paper / project, and a link to any online material (e.g. the publication itself). In paragraph or bullet-list form, make sure to specify the following: Operative System(s) in which the project was run (e.g. MacOSX Catalina or Ubuntu 18.4) The version of R / STATA / MATLAB / Julia / Python… including major and minor (e.g. R 3.6.2) Any special mentions of performance needed (e.g. “This analyses requires a machine with at least 32 GB RAM and 16 cores”) Link to any relevant data repositories Any relevant contact information, should interested people have trouble running your code When in doubt, check out the repository that Grant McDermott and Matt Burgess provided for their Science paper on Effort reduction and bycatch. 6.5.2 Sanitizing the repository When tracking a project, we’ll usually end up with many small, meaningless commit messages such as “fixed typo”, “fixed bug”, or “actually fixed bug”. While these small incremental changes allow us to revert back during the production process, in the end, we may not want to have the full list of bug fixes and meaningless commit messages visible. Thankfully, Git allows us to clean things up a bit using git rebase. Here’s an example of what your code might look like: 871adf OK, plot actually done --- newer commit 0c3317 Whoops, not yet... 87871a Plot finalized afb581 Fix this and that 4e9baa Fixed typo on x-axis d94e78 Plot model output 6394dc Fixing model --- older commit The top 6 commit messages are all related to each other. And, had you been making this plot at 9 am and not 3 pm, it would all have been a single push. Instead, we might want this to look like this: 871adf OK, plot actually done --- newer commit -┐ 0c3317 Whoops, not yet... | 87871a Plot finalized | afb581 Fix this and that | ---- Join all this into one 4e9baa Fixed typo on x-axis | d94e78 Plot model output -------------------┘ 6394dc Fixing model --- older commit In this case, we want to merge the last 6 commits into one. We want it to look like this in the end: 84d1f8 Plot model output --- newer commit (result of rebase, combining 6 messages) 6394dc Fixing model --- older commit We can do so by running the following line: Notice that I’ve specified the value 6 after the argument HEAD~. If you don’t want to count the number of commits, you can simply reference the last commit (by its hash) that you want to leave out. For example, we wanted to leave out the Fixing model commit, with hash (6394dc). Therefore, we can also run: Whichever way you go, your predetermined text editor will open. You’ll see a list of commits, containing the ones you want. (Head’s up, the older one will be on top). At the bottom of the page, you’ll see the following list of possible instructions: # Commands: # p, pick &lt;commit&gt; = use commit # r, reword &lt;commit&gt; = use commit, but edit the commit message # e, edit &lt;commit&gt; = use commit, but stop for amending # s, squash &lt;commit&gt; = use commit, but meld into previous commit # f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit&#39;s log message # x, exec &lt;command&gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with &#39;git rebase --continue&#39;) # d, drop &lt;commit&gt; = remove commit # l, label &lt;label&gt; = label current HEAD with a name # t, reset &lt;label&gt; = reset HEAD to a label # m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;] # . create a merge commit using the original merge commit&#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c &lt;commit&gt; to reword the commit message. You’ll need to preface the hash with whichever command (or shortcut) you want to use. You might want to reword a commit (i.e. remove all those “F%@#!!”), so you’ll use r. You might want to pick the head of the commit, so you’ll use p. You might want to squash multiple commits into one, so you’ll use s. In the example above, you’ll have to edit the first word of each hash to make it look like this: pick d94e78 Plot model output --- older commit s 4e9baa Fixed typo on x-axis s afb581 Fix this and that s 87871a Plot finalized s 0c3317 Whoops, not yet... s 871adf OK, plot actually done --- newer commit Now, simply save and close the file; you’ll be prompted back to your command line. The next thing to do is to give the new commit a name. Your editor will pup up. You can use the default message, or replace it with something like “Plot model output”. Save the file, close it, and push your changes. You can read much more about this on Git’s help page for Rewriting History (sounds cool, right?). "],["6.6-sharing-public-data-shiny-apps-and-tools-on-our-website.html", "6.6 Sharing public data, Shiny apps, and tools on our website", " 6.6 Sharing public data, Shiny apps, and tools on our website Any time emLab develops publicly available datasets, Shiny apps, or tools, we would love to share these on the emLab website. Any time you release a publicly available dataset, Shiny app, or other tool, please Slack or email Erin O’Reilly (eoreilly@ucsb.edu) the following information: name of the dataset app or tool, link, date published, and the emLab project it is associated with. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
