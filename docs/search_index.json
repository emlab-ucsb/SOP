[["index.html", "emLab Standard Operating Procedures Overview", " emLab Standard Operating Procedures Environmental Markets Lab (emLab) 2024-03-07 Overview This reference guide describes standard operating procedures (SOP) for emLab projects. This SOP is organized into 5 different sections: (1) File Structure, (2) Data, (3) Code, (4) High Performance Computing, and (5) Reports and Publications. Section 1 covers file structure and storage; Section 2 describes file naming and metadata standards, as well as what the emLab data directory is composed of; Section 3 highlights best practices for coding; Section 4 covers high performance computing; and Section 5 outlines aspects of the publication process from emLab affiliations to preparing a public GitHub repository. "],["1-file-structure.html", "1 File Structure", " 1 File Structure This section details emLab’s organizational stucture for Google Docs/Sheets/Slides stored on Google Shared Drive, data and other file types stored emLab’s GRIT data storage space (which we will simply refer to as “GRIT”), and code stored on GitHub. An important note: while the file structures of our Google Shared Drive and GRIT storage spaces are similar, they contain fundamentally different types of files. For the Google Shared Drive, we should store only collaborate Google Docs/Sheets/Slides. For the GRIT data storage space, we should store everything else (e.g., data, media, Micorosft Office files, PDFs, etc) "],["1.1-folder-naming.html", "1.1 Folder Naming", " 1.1 Folder Naming There are some general style conventions that should be used when naming folders. For any new folder, be descriptive but concise, avoid spaces, avoid uppercase (or camel-case), and avoid special characters other than -. Words should be separated with -, so an example folder name could be blue-paradox-paper. This folder naming convention should be used for any folder added to the Google Shared Drive, GRIT, or GitHub. "],["1.2-google-shared-drive.html", "1.2 Google Shared Drive", " 1.2 Google Shared Drive The Google Shared Drive and GRIT storage space share a very similar folder structure. The main difference is that the Google Shared Drive does not have any data folders, since all data is stored at GRIT. And of course, all files on Google Shared Drive are only collaborative Google Docs/Sheets/Slides. Everything else is stored at GRIT. 1.2.1 General Structure Google Drive |__ My Drive | |__ ... whatever files you have on your Google Drive ... |__ Shared drives |__ emLab |__ central-emlab-resources |__ communications |__ projects The emLab Google Shared Drive is organized into three main folders: central-emlab-resources: includes meeting and event information, project management guidelines, onboarding materials, information about travel reimbursements, strategy, computing, and the team roster communications: includes the blog schedule, publication and media tracking, and any Google Docs/Sheets/Slides relating to communications projects: includes information on past (archive) and current projects Google Drive |__ Shared drives |__ emLab |__ projects |__ archived-projects |__ current-project | |__ example-project | | |__ deliverables | | |__ grant-eporting | | |__ meetings-and-events | | |__ presentations | | |__ project-materials Each project folder must contain at least the following 5 folders: deliverables: final reports, paper manuscripts, other final deliverables not related to data outputs grant-reporting: grant reports for funders meetings-and-events: meeting notes, agendas, documentation for workshop/event planning presentations: any presentations created for the project project-materials: everything else that does not fit into one of these folders (i.e. drafts of methods, literature review, etc.) From here, each project can add additional folders or sub-folders as needed. Ensure that any folders or sub-folders you add to the project’s Google Shared Drive folder are also added to the folder on GRIT. "],["1.3-grit-data-storage-space.html", "1.3 GRIT data storage space", " 1.3 GRIT data storage space Generally, the GRIT data storage space has a very similar structure to the Google Shared Drive, except it also has folders for data. 1.3.1 General Structure Google Drive |__ My Drive | |__ ... whatever files you have on your Google Drive ... |__ Shared drives |__ emLab |__ central-emlab-resources |__ communications |__ data |__ projects The emLab Shared Drive is organized into five main folders: central-emlab-resources: includes meeting and event information, project management guidelines, onboarding materials, information about travel reimbursements, strategy, computing, and the team roster communications: includes the blog schedule, Adobe design projects, PowerPoint templates, photo repository, emLab logos, and publication and media tracking data: includes the centralized emLab data directory and of the commonly used datasets we work with (see Section 2.3.1 for more on this) projects: includes data and files on past (archive) and current projects 1.3.2 Project Folder Structure Google Drive |__ Shared drives |__ emLab |__ projects |__ archived-projects |__ current-project | |__ example-project | | |__ data | | |__ deliverables | | |__ grant-eporting | | |__ meetings-and-events | | |__ presentations | | |__ project-materials Each project folder must contain the following 6 folders: data: This data folder will contain a data_overview spreadsheet and all of the intermediate datasets as well as output datasets associated with the project (see Section 2.3.2 for more on this). Be sure to also add a copy of your final datasets to the emlab/datadata directory. deliverables: final reports, paper manuscripts, other final deliverables not related to data outputs grant-reporting: grant reports for funders meetings-and-events: meeting notes, agendas, documentation for workshop/event planning presentations: any presentations created for the project project-materials: everything else that does not fit into one of these folders (i.e. drafts of methods, literature review, etc.) From here, each project can add additional folders or sub-folders as needed. Ensure that any folders or sub-folders you add to the project’s Google Shared Drive folder are also added to the folder on GRIT. 1.3.3 Data Storage As stated above, there are two locations in which data can be stored. The two locations are both on GRIT, and are either the commonly shared emLab data (emLab/data) or project-specific data (e.g.,emLab/current-projects/example-project/data. This may seem confusing and redundant, but this section explains the differences between these two locations. As a short summary: example-project/data may contain raw, cleaned, intermediate, and output files for a given project, and will be used as the “workspace” while the project develops. On the other hand, emlab/data contains only (raw) input and output data from a finalized project. It also only contains datasets that researchers envision may eventually be helpful to other researchers. More detail is provided in the subsequent sections. 1.3.3.1 example-project/data All data used in a project should live in this project-specific repository. To help keep track of project data, we highly recommend creating and maintaining a data_overview Google sheet on the Google Shared Drive (see Section 2.3.2 for more information). This overview spreadsheet will provide a centralized summary of the data inputs and outputs for a project, and also allow teams to keep track of the status of adding the data to the emlab/data folder and creating the necessary metadata documentation. This repository will typically contain subfolders for raw, processed (or clean), and output, although each team might make slight modifications to this structure to suit their needs. To illustrate how each of these subfolders might be used, consider the following. A team may receive data from partners, extract data from external sources, compile survey responses, create a new dataset from a literature review, or use results from previous projects as input. These data are termed “raw data” and should never be directly modified - all of the errors, mistakes, and gremlins should be kept in the original versions. Instead, they should be processed / cleaned, and then exported as “clean data” that is actually used in analyses. The script used to do the processing / cleaning then acts as a reproducable record of everything that was done to the raw data. Suppose that a team working in Montserrat is tasked to perform a stock assessment on lobster populations and receives a database of lobster landings from the government. These data are stored as an excel spreadsheet, and will surely contain many mistakes that need to be fixed prior to running anly analyses. The team will clean the data (preferabily, using a reproducible script), and then export a new version of the data in which the mistakes have been fixed. The team will then perform the stock assessment, and produce results before reporting back. Therefore, the project-level data folder could be subdivided into raw, clean, and output folders. The first one will contain the excel file recieved from the government. The second folder will contain the cleaned data (perhaps exported as a csv), which can then be used as input for analyses within this project. The output folder will then contain the stock assessment results that might be relevant to other projects. projects |__ current-rojects |__ montserrat-project | |__ Data | |__ raw | | |__ lobster_landings_nov_2012.xslx | |__ clean | | |__ lobster_landings_nov_2012.csv | |__ output | |__ lobster_stock_assessment.csv As stated above, since the output folder could contain information relevant to other projects, this data should be made available to other emLab projects once the project is complete. To do this, any output data (and raw data if it is not already there) should be moved to the emlab/data folder, as described below. 1.3.3.2 emlab/data As a general rule, this folder contains all data used and produced by emLab projects. The idea is to make it easier for people to find data that has been used in previous projects, as well as to use previous results as inputs for new projects. To illustrate types of data that should be in the emlab/data folder, consider the following. The RAM Legacy stock assessment database is key to many projects, and was used as input in the Costello et al. 2016 “upsides” paper. The “upsides database” is an output from the Costello paper, which has then been used as input for other projects. Therefore, the emlab/data folder contains separate folders for both the RAM and upsides datasets. This large central data repository has the potential to become messy. Therefore, it is important to follow some key guidelines to store the data. All datasets in this folder should be contained within their own folders that include at minimum the data and metadata files. For example, a file structure for the two datasets mentioned above might be: emLab |__ data |__ upsides | |__ _readme_upsides.txt [the metadata] | |__ upsides.csv [the data] |__ ram | |__ _readme_RAM.txt [the metadata] | |__ RAM v4.10 [the data] | |__ RAM v4.15 [the data] | |__ RAM v4.25 [the data] | |__ RAM v4.40 [the data] |__ ... other data sets ... In the above example, the folder containing the upsides database is relatively straightforward with the metadata file and a single csv file. However, the folder containing the RAM database is more complicated as this is a dataset that is re-released every so often as a new version. Specific guidelines for organizing different types of data within the emlab/data folder are discussed in detail in Section 2. "],["1.4-github-structure.html", "1.4 GitHub Structure", " 1.4 GitHub Structure 1.4.1 Project Repository Structure The structure of each emlab repository on GitHub will likely vary depending on the needs of the project, but the following structure is suggested as a starting point. A documents (or docs) folder may be useful for storing code files that are used to generate text-based documents or presentations. Types of files that might live here include things like markdown files. A results folder may be useful for storing plots or other types of results generated by the project. Some discretion needs to be used here, as some results may actually be considered to be “processed” or “output” data. However, results in the form of figures or workspace image files might live here. A scripts folder may be useful for storing the code files that do everything from processing the raw data to running the analysis and generating outputs. A functions folder may be useful for storing the code files in which functions that are used by many scripts many be stored. Different types of projects may require more or fewer folders and these are only meant to act as suggestions. Regardless, the structure of the repository should be sufficiently organized such that it can be easily navigated and understood by others by the time the project is completed. 1.4.2 A repo inside a repo Sometimes, a project may have more than one paper or analysis sections. On some corner scenarios, we might want to have multiple “paper folders” within a “project folder”. This would imply that we will have a repo inside a repo. If that is something that makes sense for you, your project, and your team, then git submodules are your solution. If you want to read more on when / how to use submodules, visit the documentation page here. Including submodules in your workflow is simple. Here’s an example. you are working on a big project called “Blue Future”. The project has six PIs, 13 Research Specialists, two PostDocs, and three PhD Students. After a long kick-off meeting, the team realizes that the project will produce two papers and a ShinyApp. You are all determined to keep everything on the same folder, but correctly categorized and organized. As such, you go to GitHub and create the following four repositories: Blue Future Paper 1 Paper 2 ShinyApp You’ll clone the Blue Future repo into your comuter, using the usual: git clone https://github.com/emlab-ucsb/BlueFuture.git Now, instead of cloning the repos for each paper and the app into their own folder, you’ll navigate into your local BlueFuture folder. Then, instead of cloning them there, you can just do: git submodule add https://github.com/emlab-ucsb/Paper1.git This will clone the Paper 1 repo, but not without first telling the BlueFuture repo about it (just so that you don’t end up tracking things twice). You can repeat the operation for Paper2 and ShinyApp. That’s it! "],["2-data.html", "2 Data", " 2 Data Data should be managed and shared properly to make it useful in research, to promote transparency and facilitate reproducibility, and to ensure the credibility of research. This includes such practices as transforming data into a tidy format, storing data in open file formats, and providing data documentation. All emLab data, unless restricted, should be stored in the emLab Shared Drive (location dependent on type of data, see Section 2.2.3). Recommended readings: Borer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Ellis, Shannon E., and Jeffrey T. Leek. 2018. “How to Share Data for Collaboration.” The American Statistician 72 (1): 53–57. https://doi.org/10.1080/00031305.2017.1375987. Goodman, Alyssa, Alberto Pepe, Alexander W. Blocker, Christine L. Borgman, Kyle Cranmer, Merce Crosas, Rosanne Di Stefano, et al. 2014. “Ten Simple Rules for the Care and Feeding of Scientific Data.” PLOS Computational Biology 10 (4): e1003542. https://doi.org/10.1371/journal.pcbi.1003542. Hart, Edmund M., Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H. Woo, Naupaka B. Zimmerman, and Jeffrey W. Hollister. 2016. “Ten Simple Rules for Digital Data Storage.” PLOS Computational Biology 12 (10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097. Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (March): 160018. https://doi.org/10.1038/sdata.2016.18. Also see the resources available at DataONE. "],["2.1-data-file-naming.html", "2.1 Data File Naming", " 2.1 Data File Naming “Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread” - Hadley Wickham Coding styles guides provide teams with a shared language that promotes consistency, improves collaboration, and makes code easier to write as it simplifies the number of decisions we need to make on a daily basis (e.g., do I use - or _ to name this file? ). All style guides are opinionated and while they all intend to make code easier to read and write, some decisions are arbitrary. Thus, instead of us at emLab creating our own, we want to adopt the “proven and tested” Tidyverse style guide. This styles guide has good advice not only for naming files but also for naming functions, objects, and general best coding practices. We encourage all emLab members to read the guide and work towards adopting it. When it comes to file naming, best practices include: Names should be descriptive and meaningful. Many coding interfaces now have autocompletion tools, so the length of the filename is less of a concern. Avoid spaces and upper case letters. Some operating systems are case sensitive so in the interest of collaboration, let’s use only only lower case. Use _ to separate words in filenames. Do not use -, ., or any other special characters. Strive to use verbs for function names. If files should be run in a particular order, prefix them with numbers. If you’ll have more than 10 files left pad with zero (e.g., 01_get_data.R) "],["2.2-metadata.html", "2.2 Metadata", " 2.2 Metadata Metadata is data about your data. It includes information about your data’s content, structure, authors, and permissions to make your data interpretable and usable by your future self and others. EVERY data file should be accompanied by a metadata file. This includes files that people tend to overlook or think are not useful for the broader team. For example, if you’re using Google Sheets to keep track of literature or data reviews for a specific project, these documents should also have some form of accompanying metadata. 2.2.1 Metadata Standards We use “readme” style metadata, named _readme_datafilename, and stored in the same folder as the data file. Create one readme file for each data file. Download and use this template to create your readme file (when one is not already available). Name the readme _readme_datafilename and save as a text file. Format the readme so it is easy to understand (use bullets, break up information, etc.) Use a standardized date format (YYYY-MM-DD) We acknowledge that it may not always be feasible to draft robust metadata immediately when a dataset is first uploaded to the emlab/data folder. In this case, a minimal readme file can be created as a temporary placeholder that contains the following information: your name, contact info, a very brief (1-2 lines) description of the data, and a note on how you obtained them. Please refer to the Data Directory subheading for further instruction on how to incorporate this form of temporary documentation into the emLab Data Directory. For files that may be considered more “internal notes” than datasets (Google Sheets example mentioned above), please ensure that some sort of metadata is present. One alternative to a “readme” file is to create an extra tab on the Google Sheet labeled “metadata”. Here, you can include information on the column names (column 1) and their definitions (column 2). This allows collaborators and team members to easily interpret the columns and use the dataset appropriately. 2.2.2 Where to Store Metadata All readme metadata files are stored in the folder that contains the data file in the emLab Data Directory. "],["2.3-data-directories.html", "2.3 Data Directories", " 2.3 Data Directories 2.3.1 emLab Data Directory All emLab data is stored in subfolders of the Data folder on the emLab Team Drive (emlab/data). To document these data, we use the emLab Data Directory that includes key, standardized information from each readme metadata file. Every data file in the emlab/data folder has a record (row) in the emLab Data Directory. The emLab Data Directory file contains two sheets: (1) Data directory (the record and standardized documentation for each data file); (2) Metadata (information needed to populate the Data Directory, i.e. the meta-metadata) In the case of placeholder metadata (as described in the Metadata section), only the following columns should be filled out: folder, filename, contact, and summary. This (mostly blank) row serves two purposes: 1) it retains some of the searchability function for that dataset and 2) it serves as a visual reminder that those datasets are in need of more robust metadata development. Column Description Domain Climate/Energy; Land; Ocean; General; Other [drop down menu] Description A few word description (e.g. SST US 2017); max 5 words Folder Name of folder containing data Filename Name of data Year Year of publication Version Sub category of year; NA if not applicable Project Project name that used these data (can have multiple listings) or ‘General’ if widely used (e.g. FAO data), hyperlinked to Google Drive/Box folder Code Link to Github repo or wherever code is stored Data Stage raw’ if raw data; ‘final input’ for the input data used for the analysis; ‘output’ for what was used for the project and/or published [drop down menu] Filetype File extension (e.g. csv; tif; rds); note: do note include ‘.’ Citation Hyperlinked reference to publication or online resource or contact for individual/group data author URL Link to original data source Extent global; regional; national; local [drop down menu] Resolution Resolution of spatial data (in degrees) Permissions open = open source/open access; restricted = need author permission; secure = confidential data and likely involves a DUA or NDA [drop down menu] Start year Data set start year; numeric End year Data set end year; numeric Source e.g. emLab; FAO; Rare Contact Name and email of contact person in emLab who used/stored data emLab reference Hyperlinked reference to emLab publication using data (can be NA) Keywords e.g. fisheries; fire; utilities; property value; VDS; MPA; oceanography; temperature; habitat; biodiversity (up to 5 per entry, separated by semi-colons) Summary Brief description of the data (1-2 sentences). Include years for timeseries; location/spatial extent for spatial data; key variables; resolution; sampling frequency; species; etc. Notes Other relevant information about data. Initial your entry (e.g. if it was processed (e.g. subset from a larger dataset); what specifically was done; are there suspicious data points?; note if there are issues; etc.) Any time you add a new dataset to the shared emLab data folder and directory, please message the #data-streamlining Slack channel so that others on the team know about the new dataset. 2.3.2 Project-level Data Directory We highly recommend that research teams create a data_overview spreadsheet for keeping track of project-related data (i.e. a separate spreadsheet stored in the project’s Google Shared Drive data folder). This centralized document can be used to document project-relevant information and communicate to team members datasets that have already been saved. This document can then be used to guide and simplify data migration to the emLab Data Directory once the project is complete. Suggested attributes include: File name Folder name Source of data Link where data was downloaded Description of data Name of the researcher who downloaded the data Data directory entry (complete, in progress, not started, etc.) Metadata sheet (complete, in progress, not started, etc.) "],["2.4-accessing-emlab-data-in-r.html", "2.4 Accessing emLab data in R", " 2.4 Accessing emLab data in R This section will go over how to access data stored in the emLab data directories (either the shared emLab directory, or project-specific data directories) using R. We provide instructions for accessing data using R on your personal local machine, or on one of emLab’s GRIT servers. Accessing GRIT data on your local machine requires you to use the Nextcloud desktop app and set it up as described in the emLab manual. If you haven’t done so already, please complete that section. The GRIT data storage space has already been linked to our emLab quebracho server (and will eventually be to our forthcoming sequoia server). Set the base data directory at the top of your code to the location directory lives. You should be able to use the following code snippet to do this, which should work on either your local machines, or on a GRIT server such as quebracho or sequoia. Note that this code will automatically generate the correct directory path if using a GRIT server or a local Mac or Windows machine. If using a local Linux machine, you will need to manually modify this to incorporate your unique user name so that it matches the directory entered in Step 9 above. # First determine if system is quebracho or sequoia, our GRIT servers. If so, set directory appropriately data_directory_base &lt;- ifelse(Sys.info()[&quot;nodename&quot;] == &quot;quebracho&quot; | Sys.info()[&quot;nodename&quot;] == &quot;sequoia&quot;, &quot;/home/emlab&quot;, # Otherwise, set the directory for local machines based on the OS # If using Mac OS, the directory will be automatically set as follows ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Darwin&quot;, &quot;/Users/Shared/nextcloud/emLab&quot;, # If using Windows, the directory will be automatically set as follows ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Windows&quot;, &quot;G:/Shared\\ drives/nextcloud/emLab&quot;, # If using Linux, will need to manually modify the following directory path based on their user name # Replace your_username with your local machine user name &quot;/home/your_username/Nextcloud&quot;))) Now set the project-specific directory. For example: data_directory_project &lt;- file.path(data_directory_base, &quot;projects/current-projects/test-project-nextcloud/data&quot;) You should now be able to read and write data into the project data directory. For example: penguins &lt;- read.csv(file.path(data_directory_project, &quot;raw/penguins.csv&quot;)) Note that you can also always navigate to the data directory in the RStudio viewer pane. To do so, simply click the 3 dots in the viewer pane, and type in the appropriate path. "],["2.5-tidy-data.html", "2.5 Tidy Data", " 2.5 Tidy Data As researchers, we work with data from many different sources. Often these data are messy. One of the first steps of any analysis is to clean any available raw data so that you can make simple visualizations of the data, calculate simple summary statistics, look for missing or incorrect data, and eventually proceed with more involved analyses or modeling. As part of the data cleaning process, we recommend getting all data into a “tidy” format (also known as “long” format, as opposed to “wide” format). According to the Tidy Data Guide by Garrett Grolemund and Hadley Wickham, tidy data is defined as: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. These three features of tidy data can be seen in the following figure, also from the Tidy Data Guide: Once data are in this format, it makes subsequent visualization and analysis much easier. If you’ve ever worked with a file that has a separate column for each year (an example of “wide” format data), you know how hard that type of data format is to work with! As always, we recommend keeping a backup copy of the raw data you obtained from the original source, and using a reproducible script for transforming these data into a tidy data format. 2.5.1 Recommended Resources We highly recommend the chapter on Tidy Data from the book R for Data Science by Garrett Grolemund and Hadley Wickham. This guide is geared towards R users and provides helpful tips for transforming and working with data in R, but the concepts should be broadly applicable to other languages as well. For tips specific to Python, we recommend a blogpost by Jean-Nicholas Hould titled Tidy Data in Python. "],["2.6-data-formats.html", "2.6 Data Formats", " 2.6 Data Formats Data should preferably be stored and/or archived in open formats. Open formats maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them. The exact format will vary depending on the file type, but some common examples are comma separated value (.csv) as opposed to closed format Excel spreadsheets (.xls) for tabular data, and .pdf or .odt (Open document format for office applications) as opposed to Word documents for final versions of reports. When saving final geospatial products, we recommend geopackage (.gpkg) which has several advantages over the commonly used shapefile format (more here): Single file Open format (shapefile is ESRI-dependent) Fewer limitations: e.g. no limits on attribute file names (shapefile has max. 254 characters), max file size 140 TB! (shapefile max. 4GB). Geopackage files can be written and read in R, QGIS, and ArcGIS. "],["2.7-data-use-agreements-and-confidential-data.html", "2.7 Data Use Agreements and Confidential Data", " 2.7 Data Use Agreements and Confidential Data 2.7.1 The process for establishing a Data Use Agreement or Non-Disclosure Agreement At project launch, the project manager and the rest of the project team should determine if a Data Use Agreement (DUA; common) or Non-Disclosure Agreement (NDA; not common) is necessary. Any project that will involve the use and sharing of data that is not publicly available should establish a DUA or NDA. Start this process right away. DUA is preferred if possible; only do NDA if necessary or requested by partner. These agreements should go through UCSB’s Office of Technology &amp; Industry Alliances (TIA). This page provides guidelines for establishing a DUA. The first step in establishing a DUA is to fill out a DUA Request Form and send it to Jenna Nakano (nakano@tia.ucsb.edu). This page provides guidelines for establishing an NDA. The first step in establishing an NDA is to fill out a NDA Request Form and send it to Jenna Nakano (nakano@tia.ucsb.edu). CC emLab’s Amanda Kelley (aekelley@ucsb.edu) on all emails relating to the DUA or NDA. Once a request form has been sent, TIA will help produce standardized agreements that are based on answers to these forms. Alternatively, TIA can also review DUAs or NDAs that partners share. 2.7.2 Data storage options emLab is generally happy to work with partners to determine the most appropriate method for data storage. Some partners may have specific data storage requirements that will be laid out in the DUA or NDA. If the DUA or NDA does not have specific data storage requirements, we recommend one of the following three approaches depending on how sensitive the data are: For data that are not confidential or sensitive, data should be stored in a project-specific directory on the emLab Team Drive. Only emLab PIs and full-time emLab staff will have default access to this data directory. Any additional access for postdocs, students, or other external collaborators will only be granted on an as-needed basis and only after the collaborator has read the DUA or NDA. This option is used for the vast majority of emLab projects. For confidential or sensitive data, the primary recommended approach is the UCSB Knot Cluster through the Center for Scientific Computing. We recommend using this approach if your DUA or NDA allows for it. To set this up, use the request form here. Nathan (Fuzzy) Rogers (Research Computing Administrator, fuz@mrl.ucsb.edu) and Paul Weakliem (CNSI Research Computing Support, weakliem@cnsi.ucsb.edu) are good resources for questions. Anyone storing sensitive data with the knot cluster should ensure that UCSB locks the data so that they remain private. For confidential or sensitive data, the second option is storing data on the Secure Compute Research Environment (SCRE) at UCSB’s North Hall Data Center. The SCRE “is a private, secure, virtual environment for researchers to remotely analyze sensitive data, create research results, and output results and analyses.” We only recommend this approach if your DUA or NDA requires data be stored in a secure facility like the North Hall Data Center Setting up an SCRE gives you access to a secure virtual desktop that comes pre-loaded with applications such as R and R Studio. You can make a request for an SCRE using the request form here. UCSB IT will help set this up. You can follow up with questions at scre-support@ets.ucsb.edu Requests are usually fulfilled within one week. Further information can be found in the SCRE user guide. Jennifer Mehl (Information Security Analyst, jennifer.mehl@ucsb.edu) is another good resource for questions. The SCRE has a number of important limitations: it is relatively slow; it is very difficult to access for non-UCSB collaborators; it can only be set up after the NDA/DUA is established; it can be difficult to install Stata and may require an individual license; any non-standard R packages need to be installed manually by a UCSB person managing the SCRE 2.7.3 Other best practices Regardless of the data storage option chosen, we recommend several additional best practices: High-level metadata for all datasets should be added to the _emlab_data_directory. See this section in the emLab SOP for further description of the emLab data directory. This will help emLab be internally transparent in how we are using data for different projects, even if the entire group doesn’t have access to the data. For datasets that are confidential, ensure that the “Permissions” column is set to “Secure = confidential data and likely involves a DUA or NDA.” Researchers should consider anonymizing individual-level data before publicly releasing the data (see this R package as one example for how to do this). "],["3-code.html", "3 Code", " 3 Code All data processing and analysis should be performed with code (i.e., avoid spreadsheets), and all code should be packaged in scripts that are version controlled and follow a style guide. Using code and scripts allows for better organization, documentation, and reproducibility of analysis workflows. All emLab code should be stored in the emLab GitHub account. Recommended readings: Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk About Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928. Stodden, Victoria, and Sheila Miguez. 2014. “Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research.” Journal of Open Research Software 2 (1): e21. https://doi.org/10.5334/jors.ay. Wilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745. https://doi.org/10.1371/journal.pbio.1001745. Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510. "],["3.1-scripts-and-version-control.html", "3.1 Scripts and Version Control", " 3.1 Scripts and Version Control Code should be crafted according to the following guidelines: Use scripts Document scripts, but not too much Organize scripts consistently (see format below) Use Git to version control scripts Make atomic Git commits (see description below) Script files should be documented and organized in such a way to enhance readability and comprehension. For example, use a standardized header for general documentation and sections to make it easier to understand and find specific code of interest. Code should also be self-documenting as much as possible. Additionally, use relative filepaths for importing and exporting objects. Scripts should also be modular by focusing on one general task. For example, use one script for cleaning data, another script for visualizing data, etc. A makefile can then be used to document the analysis workflow. There is an art to this organization, so just keep in mind the general principle of making code easy to understand, for your future self and for others. Here is an example template for R scripts: # ============================================================================= # Name: script.R # Description: Visualizes data # # Inputs: data.csv # Outputs: graph.png # # Notes: - Use a diverging color palette for best results # - Output format can be changed as needed # ============================================================================= # Set up environment ---------------------------------------------------------- library(tidyverse) # Set path for Google Drive filestream based on OS type # Note - this will work for Windows and Mac machines. # If you use Linux, you will need to set your own path to where Google Drive filestream lives. team_path &lt;- ifelse(Sys.info()[&quot;sysname&quot;]==&quot;Windows&quot;,&quot;G:/&quot;,&quot;/Volumes/GoogleDrive/&quot;) # Next, set the path for data directory based on whether project is current or archived. # Note that if you use a different Shared Drive file structure than the one recommended in the &quot;File Structure&quot; section, you will need to manually define your data path. # You should always double-check the automatically generated paths in order to ensure they point to the correct directory. # First, set the name of your project project_name &lt;- &quot;my-project&quot; # This will automatically determine if the project exists in the &quot;current-projects&quot; or &quot;archived-projects&quot; Shared Drive folder, and set the appropriate data path accordingly. data_path &lt;- ifelse(dir.exists(paste0(team_path,&quot;Shared drives/emlab/projects/current-projects/&quot;,project_name)), paste0(team_path,&quot;Shared drives/emlab/projects/current-projects/&quot;,project_name,&quot;/data/&quot;), (paste0(team_path,&quot;Shared drives/emlab/projects/archived-projects/&quot;,project_name,&quot;/data/&quot;))) # Import data ----------------------------------------------------------------- # Load data from Shared Drive using appropriate data path my_raw_data &lt;- read_csv(paste0(data_path,&quot;raw/my_raw_data.csv&quot;)) # Process data ---------------------------------------------------------------- # Analyze data ---------------------------------------------------------------- # Visualize results ----------------------------------------------------------- # Save results ---------------------------------------------------------------- Git tracks changes in code line-by-line with the use of commits. Commits should be atomic by documenting single, specific changes in code as opposed to multiple, unrelated changes. Atomic commits can be small or large depending on the change being made, and they enable easier code review and reversion. Git commit messages should be informative and follow a certain style, such as the guide found here. There is also an art to the version control process, so just keep in mind the general principle of making atomic commits. More advanced workflows for using Git and GitHub, such as using pull requests or branches, will vary from project to project. It is important that the members of each project agree to and follow a specific workflow to ensure that collaboration is effective and efficient. "],["3.2-style-guide.html", "3.2 Style Guide", " 3.2 Style Guide At emLab, we recommend using a consistent code style for each of the different programming languages we use. Recommended coding styles for a particular language are collated in what is known as a “style guide”. Style guides typically include standardized ways of naming script files, defining functions and variables, commenting code, etc. While emLab does not mandate the use of style guides, having a consistent code style allows all emLab staff to easily understand each other’s code, collaborate on projects, and jump in on new projects. We consider this an important aspect of collaboration, reproducibility, and transparency. Rather than re-invent the wheel, we leverage existing code style guidelines for each of the languages we use. Below is a summary of the code style guidelines we recommend for various languages: R - Tidyverse Style Guide, by Hadley Wickham Python - Google’s Python Style Guide Stata - Suggestions on Stata programming style, by Nicolas J. Cox Other languages - Google style guides for other languages "],["3.3-reproducibility.html", "3.3 Reproducibility", " 3.3 Reproducibility Prioritizing reproducibility when writing code code not only fosters collaboration with others during a project, but also makes it easier for users in the future (including yourself!) to make changes and rerun analyses as new data become available. Some useful tools and practices include: Commenting code: Adding brief but detailed comments to your scripts that document what your code does and how it works will help others understand and use your scripts. At the top of your scripts, describe the purpose of the code as well as its necessary inputs, required packages, and outputs. File paths: Avoid writing file paths that only work on your computer. Where possible, use relative file paths instead of absolute files paths so your code can be run by different users or operating systems. For R users, using R Projects and/or the here package are great ways to help implement this practice. Functions: If you find yourself copying and pasting similar blocks of code over and over to repeat tasks, turn it into a function! R Markdown: R users can take advantage of R Markdown for a coding format that seamlessly integrates sections of text alongside code chunks. R Markdown also enables you to transform your code and text into report-friendly formats such as PDFs or Word documents. Git and GitHub: As described above, Git tracks changes to files line-by-line in commits that are attributable to each team member working on the project. GitHub then compiles the history of any Git-tracked file online, and synchronizes the work of all collaborators in a central repository. Using Git and GitHub allow multiple people to code collaboratively, examine changes as they occur, and restore prior versions of files if necessary. We also recommend managing package dependencies and creating a reproducible coding pipeline. These two aspects are discussed in more detail below. 3.3.1 Package dependencies A key aspect of reproducibility is package dependency management. For example, your code may run perfectly when using an old version of a particular package, but might fail when using the latest version of that package. In this case, even if someone has the exact code you used, it would fail to run on their machine if they are using the latest version of that package. It is therefore important that along with providing the code you use, you should also somehow provide the packages and package versions you use. If using R, we recommend using renv package for managing package dependencies. renv was developed by the R Studio (now Posit) team. It is used at the project-level, and can thus be included as part of your GitHub respotitory. To set up your project to use renv, first initialize it by running renv::init(). You will then be able “snapshot” the packages and package version you are using in a project (by running renv::snapshot), and save that snapshot so that yourself or others can “restore” to that snapshot and use the exact same packages and package versions (by running renv::restore). This will ensure that your code will run on other machines, even if new versions of packages come out that are not backwards compatible. 3.3.2 Coding pipelines Often, a project contains multiple script files that are each run separately, but which produce or use interconnected inputs and outputs through a coding pipeline. They must therefore be run in the correct order, and any time a change is made to an upstream script or file, all downstream scripts may need to be run for their outputs to stay up-to-date. This presents a challenge to reproducibility, and also a challenge to anyone trying to work with or review complicated coding pipelines. Additionally, not knowing which files and scripts are up-to-date could mean that long-running operations are re-run unnecessarily. Consider the following example, from Juan Carlos Villaseñor-Derbez’s excellent tutorial: Make: End-to-end reproducibility. Consider you have the following project file structure in a GitHub repository, where clean_data.R is run first and processes the raw data file raw_data.csv. When this is run, clean_data.R produces the cleaned data file clean_data.csv. plot_data.R is run next, which uses clean_data.csv and produces the final output figure1.png. data |_raw_data.csv |_clean_data.csv scripts |_clean_data.R |_plot_data.R results |_figure1.png |_raw_data.csv |_clean_data.csv These scripts must be run in the correct order, and any time changes are made to upstream data or scripts, downstream scripts must be run to update downstream data and results. To ensure reproducibility of your code, we recommend everyone employ one of the three following options when using a coding pipeline, with the four options in ascending order of recommendation from “minimum required” to “best practice.” We provide a best practice for non-R code, as well as a best practice for R code. Option 1 (minimum required): Provide clear documentation in the repo’s README.md file. This documentation should provide the complete file structure, including both scripts and data files, and should provide a narrative description of the order in which the scripts should be run, as well as which files each script uses and produces. Option 2 (better practice): Create a run_all.R script that runs all scripts in the correct order, and can be run to fully reproduce the entire project repo’s outputs. This run_all.R script should be described in README.md. Using the example above, this script could contain the following: # Run all scripts # This script runs all the code in my project, from scratch source(&quot;scripts/clean_data.R&quot;) # To clean the data source(&quot;scripts/plot_data.R&quot;) # To plot the data Option 3 (best practice for non-R code): For non-R code, we recommend using the make package. This uses a Makefile to fully describe the coding pipeline through a standardized syntax of targets (output files that need to be created), prerequisites (file dependencies), and commands (how to go from prerequisites to targets). The advantage of this approach is that it automatically runs scripts in the correct order, keeps track of when changes are made to scripts or data files, and it only runs the necessary scripts to ensure that all outputs are up-to-date. If using make, you should describe how to use it in the repo’s README.md. The Makefile uses the following syntax: target: prerequisites command Therefore, in our example, the Makefile would simply be the following, and the single command make would execute this: results/figure1.png: scripts/plot_data.R data/clean_data.csv Rscript scripts/plot_data.R data/clean_data.csv: scripts/clean_data.R data/raw_data.csv Rscript scripts/clean_data.R Please refer to Juan Carlos Villaseñor-Derbez’s very helpful tutorial for more information: Make: End-to-end reproducibility. Option 4 (best practice for R code): For R code, we recommend using the targets package. Targets is a Make-like pipeline tool that has been specifically developed for R. Using targets means that anytime an upstream change is made to the data or models, all downstream components of the data processing and analysis will be re-run automatically when the targets::tar_make() command is run. It also means that once components of the analysis have already been run and are up-to-date, they will not need to be re-run. All interim objects are cached in a _targets directory. This directory can either be within in the project directory when the objects are small (e.g., within the GitHub repo), or can be set to another directory when objects are larger (e.g., on Google Shared Drive or Google Cloud Storage). Targets also has a built-in parallel processing option, which can allow long-running components of the analysis to be run in parallel. It also plays nicely with renv for package management. For further information, the targets package documentation is a great place to start. We also recommend the presentation (and associated GitHub repository) developed by Tracey Mangin for R-Ladies Santa Barbara as another great primer. "],["3.4-internal-code-review.html", "3.4 Internal code review", " 3.4 Internal code review Peer-review of each other’s code is a great method for mutual learning through exposure to different coding styles and packages, ensuring reproducibility, catching any bugs, and uncovering opportunities for doing things better. We recognize that this takes time, but we believe that the time commitment is worth it for us to all become better coders and write better code. As a best practice, we recommend that for any project that has at least two researchers, there should be a systematic review of any major coding elements by the researcher who did not initially write the code. By following the best practices outlined in the Coding Pipeline section of this SOP, the original coder can ensure that the reviewing coder knows exactly which pieces of code are critical for their review. Code reviews should be respectful and collegial in manner. The goal is to make all of our work better and make all us better coders, not to criticize or single anyone out. The original coder and reviewing coder should coordinate on the best method for feedback, whether that is through direct pull requests to the code, an email outlining suggestions, or a Google doc outlining suggestions. For further guidance on effective code reviewing techniques and guiding principles, we recommend using the Tidyteam code review principles. These were developed by the tidyteam for maintaining packages such as those found in the tidyverse and tidymodels. These guidelines are based on Google’s Code Review Developer Guide, another helpful resource. "],["3.5-exploratory-data-analysis.html", "3.5 Exploratory Data Analysis", " 3.5 Exploratory Data Analysis Much of the work emLab does is Exploratory Data Analysis (EDA), especially during the beginning stages of a project when we are familiarizing ourselves we new datasets and rapidly prototyping data wrangling and modeling approaches. The following quote from Hadley Wickham’s R for Data Science describes EDA as: EDA is an iterative cycle. You: 1) Generate questions about your data; 2) Search for answers by visualising, transforming, and modelling your data; 3 Use what you learn to refine your questions and/or generate new questions. EDA is a creative and exploratory process without defined rules. Hadley Wickham’s chapter on EDA is an excellent place to start for ideas, but we encourage creativity and exploration during this phase of a project. However, we recommend as a best practice that all EDA should include an “Interpretations, questions, and new ideas” section. The researcher doing EDA is poised in an excellent position for providing initial interpretations of the data, raising outstanding questions about the data, and generating novel insights and potential research questions. This section should contain the following information: Interpretations about what you found. Does it match our intuition? Is there anything surprising? Questions about the data. Are there any questions about how to use or interpret the data? Are there any potential problems with the data or analysis? New insights and research questions. Have you uncovered any new insights about the data or research? Have you discovered any new research questions that might be worth pursuing? By including a section in each EDA analysis that discusses these aspects, we can build insight generation into our workflows. This information should be in an easy to find location on the Team Drive, such as in a markdown-reports directory that contains EDA PDFs generated by R Markdown. This way, other members of the project team can read it, digest it, and respond with further ideas. "],["4-high-performance-computing.html", "4 High Performance Computing", " 4 High Performance Computing Certain analysis use cases require high performance computing resources: big data parallel computing lengthy computation times restricted-use data For analyses involving big data or models that take a long time to estimate, a single laptop or desktop computer is often not powerful enough or becomes inconvenient to use. Additionally, for analyses involving restricted-use data, such as datasets containing personally identifiable information, data use agreements typically stipulate that the data should be stored and analyzed in a secure manner. In these cases, you should use the high performance computing resources available to emLab, including cloud computing through Google Cloud Platform and the UCSB server clusters. Cloud computing incurs costs but is flexible whereas the UCSB server clusters are free but have some limitations, such as job queues. When to use Google Cloud Platform: need maximum computational flexibility When to use UCSB server clusters: costs are a concern using Stata using restricted-use data (depends on data use agreement) "],["4.1-google-cloud-platform.html", "4.1 Google Cloud Platform", " 4.1 Google Cloud Platform Google Cloud Platform (GCP) is a suite of cloud-based products that work together to provide robust and seamless solutions to high performance computing, big data storage, data analytics, machine learning and more. The platform is built on Google’s internal infrastructure and it’s known for it’s reliability, flexibility, speed, and a relatively low cost “pay-as-you” model. At emLab we mainly use three of GCP’s products: Cloud Storage, BigQuery, and Compute Engine and each year we have a limited amount of credits to cover the costs of using these tools for projects that require the storage and use of very large datasets, projects that require large computational power, or those that use Global Fishing Watch data. When it comes to high performance computing, Compute Engine is a very useful tool. It allows us to easily create custom-made virtual machines with the storage, memory, and number of cores needed for a given task. Virtual machines can run public images of Linux, Windows Server, and can also be used to deploy Docker containers. Starting, stopping, and deleting virtual machines is easy and fast which means we have full control on the amount of resources we use and get billed for. To get up and running with a virtual machine, Grant McDermott (SFG alumn and fellow) wrote this really good step by step tutorial. Here you will learn how to create, start, connect to, and stop a virtual machine in Compute Engine and how to install Rstudio server and Git. Importantly, you will also find a link that walks you through the installation of Google Cloud SDK command line utility (gcloud) which is a prerequisite to be able to speak to your virtual machine from your local terminal. When you install gcloud and autenthicate your credentials you will be able to set emlab-gcp as your project which will link you to emlab’s billing account. If you have not joined emlab-gcp please get in touch with and we will set you up! General guidelines for creating and running virtual machines: Give your VM a descriptive name associated with the specific project you will be using it for. Give your VM a static IP address. That way you can add it to your bookmarks and access it easily. Always turn off your VM when not it use. Remember we get charged for every minute it is on. Delete the VM once the project is finished. That way we keep things tidy. 4.1.1 Connecting to emLab’s Shared Drive A key consideration when using virtual machines is being able to access data stored elsewhere instead of having to copy data to the VMs hardrive. At emLab we use Google Drive as the central repository for datasets and project files, and fortunately, there are tools to connect to it from VMs created in Compute Engine. For VMs using GUI interfaces (e.g., Windows or MacOS), one can simply use filestream as one would locally. However, for headless VMs such as those running Ubuntu which - we create often to run Rstudio server - we need to use a FUSE filesystem over Google Drive called google-drive-ocamlfuse. At of November 2019, this workflow works for zesty and xenial distributions of Ubuntu. In a VM running Ubuntu, follow the installation instructions via PPA repository found here. sudo add-apt-repository ppa:alessandro-strada/google-drive-ocamlfuse-beta sudo apt-get update sudo apt-get install google-drive-ocamlfuse After installation you need to authorize google-drive-ocamlfuse and create a label for the connection. Labels are useful if you want to mount your personal drive as well as emLab’s Shared Drive. To authorize and create a label for our shared drive run: google-drive-ocamlfuse -headless -label emlab_drive -id ##yourClientID##.apps.googleusercontent.com -secret ###yoursecret##### Copy and paste the clientID and secret which can be found here under the file-stream OAuth 2.0 client ID. You will get an output like this: Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=##yourClientID##.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&amp;response_type=code&amp;access_type=offline&amp;approval_prompt=force Follow the prompt, grant access, and copy and paste the verification code in the terminal prompt. You should see a message saying the token was succesfully authorized. The last step to mount the team drive is to save the Shared Drive ID to the corresponding config file. This step is not necessary if you want to connect to your personal drive only. Open the config file in ~/.gdfuse/emlab_drive/config and look for the team_drive_id setting. Add our Team Drive ID (0AHyeeMXswgGLUk9PVA) and save the file. Now you are ready to mount the drive to a local folder! google-drive-ocamlfuse -label emlab_drive mountPoint "],["4.2-ucsb-server-clusters.html", "4.2 UCSB Server Clusters", " 4.2 UCSB Server Clusters The Center for Scientific Computing at UCSB provides resources and support for research involving high performance computing, including multiple server clusters for storing and analyzing data. Find out more information at http://csc.cnsi.ucsb.edu/. The clusters should be used if cloud computing costs are a concern. The cluster computing resources are free to emLab researchers, but they are a shared resource among UCSB researchers and involve job queues that can potentially delay analyses. However, most of the analyses typical of emLab projects require a small amount of resources relative to other users of the clusters. Additionally, the clusters should typically be used if using restricted-use data in research, though this depends on the terms of the data use agreement. Be sure to restrict access to any sensitive data on the clusters by changing the folder and file permissions appropriately. Some data providers may prefer to use cloud computing services for sensitive data because it may give them greater control over the uses of their data. If needed, these issues about where data can be stored and analyzed should be negotiated and resolved in data use agreements. A user guide for the clusters can be found at https://emlab-ucsb.github.io/cluster-guide/. "],["5-reports-and-publications.html", "5 Reports and Publications", " 5 Reports and Publications This section describes best practices related to emLab reports and publications. "],["5.1-emlab-affiliation.html", "5.1 emLab Affiliation", " 5.1 emLab Affiliation Since emLab is joint between MSI and Bren and also its own research entity, we recommend emLab staff use the following affiliations for any emLab publication: Marine Science Institute, University of California, Santa Barbara, Santa Barbara, CA, USA Bren School of Environmental Science &amp; Management, University of California, Santa Barbara, Santa Barbara, CA, USA Environmental Markets Lab, University of California, Santa Barbara, Santa Barbara, CA, USA Note that journals may list affiliation addresses slightly different than what is listed above. "],["5.2-reports.html", "5.2 Reports", " 5.2 Reports Many final deliverables for our projects include creating reports. To aid in the final report development, please see the Report template for information on a draft cover page, inside page, logos to use, and suggested citation. To ensure future searchability, please upload all final reports to Zenodo - a public repository that creates a DOI and link for all uploaded information - if the report can be made publicly available. If you don’t have an account, you can sign up here with your email, GitHub account, or ORCID. We have created an emLab repository for all of our reports to live. To upload to the emLab community repository, use this link and fill out all of the required information. "],["5.3-author-contribution.html", "5.3 Author Contribution", " 5.3 Author Contribution Transparency in authors’ contribution is a critical component of open science. Determining authorship is ultimately the responsibility of the project leads (i.e. principal investigator and/or first author), but because author contribution is not always straightforward, we follow McNutt et al. (2018) and others in recommending the Contributor Roles Taxonomy (CRediT) system. For a simplified version of this most relevant to our project infrastructure, feel free to use and modify our Author Contribution Template, which allows individuals to identify their contributions or to opt-out of participation in a paper. We recommend discussing authorship expectations early in a project to avoid future complications. "],["5.4-making-your-data-publicly-available.html", "5.4 Making Your Data Publicly Available", " 5.4 Making Your Data Publicly Available Generally, emLab researchers should be prepared to share a public GitHub repository (see Section 6.4) and a Dryad data repository with publications. The repositories contain code scripts and data (respectively) needed to conduct the study. Increasingly, journals are requiring both sets of documentation at the point of submission or publication. Dryad is an open-access repository of research data that makes data searchable, freely accessible, and citable. Data repositories do not need to be associated with a publication. Dryad is free for UCSB affiliates with a UCSB NetID. The following steps document the process for creating a Dryad repository. Review Dryad’s Best Practices website for more useful information. If you do not already have one, visit the ORCID site to obtain an ORCID identifier. You will need this to deposit data on Dryad. Use your ORCID username and password to sign into Dryad. Under My Datasets, select “+ Start New Dataset.” Fill in the required fields and describe your dataset. If your data is related to a manuscript in progress or a published article, you will be asked to provide the Manuscript Number or DOI. Upload your files. Note that while you cannot load a folder to Dryad, you can load a .zip file that includes a folder structure. This approach is suggested as it allows for improved organization. Review and submit. In this phase you can choose to “Enable Private for Peer Review,” which keeps your dataset private during your article’s review period. You will have access to a private dataset download URL that you can share with collaborators or the journal. If you make this selection, your dataset will not enter curation or be published. If you do not select “Enable Private for Peer Review,” you will submit your dataset to Dryad for curation. Skip to step 9 if you did not “Enable Private for Peer Review.” If you chose “Enable Private for Peer Review,” copy the URL to share with collaborators or the journal. Those who use this link may be asked to provide an email address in order to obtain the dataset. Note that in this case, users will receive an email from Dryad with instructions for how to download the dataset – notify users to check their Spam folder for this email. Once the manuscript is accepted, you can go back to your dataset and submit it to Dryad for curation. Make sure to incorporate any relevant changes that occurred during the revision period. During curation, Dryad will check your submission to ensure the validity of files and metadata. Dryad may contact you questions, suggestions, and/or identified problems. Once the dataset is approved, the Dryad DOI is officially registered and made public. Note that you can contact Dryad in order to delay the publication of your data until your publication date. Include DOI in your manuscript’s Data Availability statement, or consider citing it the References section. "],["5.5-preparing-a-public-github-repository.html", "5.5 Preparing a Public GitHub Repository", " 5.5 Preparing a Public GitHub Repository As with data, we strive to make all our code available. This provides a roadmap of converting input data into tangible results, which may be of interest for external people seeking to replicate our study or for internal emLabers seeking to understand what someone did a few years ago. 5.5.1 Documentation One of the most important things to include in the repo is a README.md file. This will be automatically displayed as rendered markdown on GitHub, and should provide a simple explanation of what’s in the repo, how to run it, and how it was run in the past. If possible / necessary, you might want to include a file structure (Take a look at using startR::create_readme() for automating this). If relevant, you might want to include the title of the paper / project, and a link to any online material (e.g. the publication itself). In paragraph or bullet-list form, make sure to specify the following: Operative System(s) in which the project was run (e.g. MacOSX Catalina or Ubuntu 18.4) The version of R / STATA / MATLAB / Julia / Python… including major and minor (e.g. R 3.6.2) Any special mentions of performance needed (e.g. “This analyses requires a machine with at least 32 GB RAM and 16 cores”) Link to any relevant data repositories Any relevant contact information, should interested people have trouble running your code When in doubt, check out the repository that Grant McDermott and Matt Burgess provided for their Science paper on Effort reduction and bycatch. 5.5.2 Sanitizing the repository When tracking a project, we’ll usually end up with many small, meaningless commit messages such as “fixed typo”, “fixed bug”, or “actually fixed bug”. While these small incremental changes allow us to revert back during the production process, in the end, we may not want to have the full list of bug fixes and meaningless commit messages visible. Thankfully, Git allows us to clean things up a bit using git rebase. Here’s an example of what your code might look like: 871adf OK, plot actually done --- newer commit 0c3317 Whoops, not yet... 87871a Plot finalized afb581 Fix this and that 4e9baa Fixed typo on x-axis d94e78 Plot model output 6394dc Fixing model --- older commit The top 6 commit messages are all related to each other. And, had you been making this plot at 9 am and not 3 pm, it would all have been a single push. Instead, we might want this to look like this: 871adf OK, plot actually done --- newer commit -┐ 0c3317 Whoops, not yet... | 87871a Plot finalized | afb581 Fix this and that | ---- Join all this into one 4e9baa Fixed typo on x-axis | d94e78 Plot model output -------------------┘ 6394dc Fixing model --- older commit In this case, we want to merge the last 6 commits into one. We want it to look like this in the end: 84d1f8 Plot model output --- newer commit (result of rebase, combining 6 messages) 6394dc Fixing model --- older commit We can do so by running the following line: Notice that I’ve specified the value 6 after the argument HEAD~. If you don’t want to count the number of commits, you can simply reference the last commit (by its hash) that you want to leave out. For example, we wanted to leave out the Fixing model commit, with hash (6394dc). Therefore, we can also run: Whichever way you go, your predetermined text editor will open. You’ll see a list of commits, containing the ones you want. (Head’s up, the older one will be on top). At the bottom of the page, you’ll see the following list of possible instructions: # Commands: # p, pick &lt;commit&gt; = use commit # r, reword &lt;commit&gt; = use commit, but edit the commit message # e, edit &lt;commit&gt; = use commit, but stop for amending # s, squash &lt;commit&gt; = use commit, but meld into previous commit # f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit&#39;s log message # x, exec &lt;command&gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with &#39;git rebase --continue&#39;) # d, drop &lt;commit&gt; = remove commit # l, label &lt;label&gt; = label current HEAD with a name # t, reset &lt;label&gt; = reset HEAD to a label # m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;] # . create a merge commit using the original merge commit&#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c &lt;commit&gt; to reword the commit message. You’ll need to preface the hash with whichever command (or shortcut) you want to use. You might want to reword a commit (i.e. remove all those “F%@#!!”), so you’ll use r. You might want to pick the head of the commit, so you’ll use p. You might want to squash multiple commits into one, so you’ll use s. In the example above, you’ll have to edit the first word of each hash to make it look like this: pick d94e78 Plot model output --- older commit s 4e9baa Fixed typo on x-axis s afb581 Fix this and that s 87871a Plot finalized s 0c3317 Whoops, not yet... s 871adf OK, plot actually done --- newer commit Now, simply save and close the file; you’ll be prompted back to your command line. The next thing to do is to give the new commit a name. Your editor will pup up. You can use the default message, or replace it with something like “Plot model output”. Save the file, close it, and push your changes. You can read much more about this on Git’s help page for Rewriting History (sounds cool, right?). "],["5.6-sharing-public-data-shiny-apps-and-tools-on-our-website.html", "5.6 Sharing public data, Shiny apps, and tools on our website", " 5.6 Sharing public data, Shiny apps, and tools on our website Any time emLab develops publicly available datasets, Shiny apps, or tools, we would love to share these on the emLab website. Any time you release a publicly available dataset, Shiny app, or other tool, please Slack or email Erin O’Reilly (eoreilly@ucsb.edu) the following information: name of the dataset app or tool, link, date published, and the emLab project it is associated with. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
